{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:31.149061Z",
     "end_time": "2023-04-24T18:14:33.702350Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import losses, optimizers, metrics, callbacks, Model, layers, backend as K\n",
    "import SpeechModels\n",
    "from augment_layers import FreqMaskLayer, TimeMaskLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:33.704339Z",
     "end_time": "2023-04-24T18:14:34.295702Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "N_CLASS = 12\n",
    "MAX_EPOCHS = 200\n",
    "TEST_PATH = \"tensorflow-speech-recognition-challenge/test\"\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:34.297695Z",
     "end_time": "2023-04-24T18:14:34.312645Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45586 files belonging to 12 classes.\n",
      "Found 6513 files belonging to 12 classes.\n",
      "Found 13024 files belonging to 12 classes.\n",
      "label names: ['down' 'go' 'left' 'no' 'off' 'on' 'right' 'silence' 'stop' 'unknown'\n",
      " 'up' 'yes']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/train\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/val\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/test\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print(\"label names:\", label_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:34.316631Z",
     "end_time": "2023-04-24T18:14:39.933557Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 158538 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=TEST_PATH,\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:39.935550Z",
     "end_time": "2023-04-24T18:14:54.294727Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['audio']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.class_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:54.296721Z",
     "end_time": "2023-04-24T18:14:54.310721Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorSpec(shape=(None, 16000, None), dtype=tf.float32, name=None),\n TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_ds.concatenate(val_ds)\n",
    "train_dataset.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:54.311717Z",
     "end_time": "2023-04-24T18:14:54.355577Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorSpec(shape=(None, 16000, None), dtype=tf.float32, name=None),\n TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = test_ds\n",
    "validation_dataset.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:54.327670Z",
     "end_time": "2023-04-24T18:14:54.363549Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 16000)]      0           []                               \n",
      "                                                                                                  \n",
      " normalized_spectrogram_model (  (None, 125, 80)     0           ['input[0][0]']                  \n",
      " Functional)                                                                                      \n",
      "                                                                                                  \n",
      " freq_mask_layer (FreqMaskLayer  (None, 125, 80)     0           ['normalized_spectrogram_model[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 125, 80, 1)   0           ['freq_mask_layer[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 125, 80, 10)  60          ['tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 125, 80, 10)  40         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 125, 80, 1)   51          ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 125, 80, 1)  4           ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " squeeze_last_dim (Lambda)      (None, 125, 80)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 125, 128)     74240       ['squeeze_last_dim[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 125, 128)    98816       ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 128)          0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 125)          0           ['dense[0][0]',                  \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " attSoftmax (Softmax)           (None, 125)          0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 128)          0           ['attSoftmax[0][0]',             \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 12)           396         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 200,455\n",
      "Trainable params: 200,433\n",
      "Non-trainable params: 22\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def squeeze(audio, labels):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, labels\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(squeeze, tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(squeeze, tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(squeeze, tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def create_model(freq=False, time=False):\n",
    "    m = SpeechModels.get_melspec_model(iLen=16000)\n",
    "    m.trainable = False\n",
    "    inputs, outputs = m.inputs, m.outputs\n",
    "\n",
    "    x = m(inputs)\n",
    "    if freq:\n",
    "        x = FreqMaskLayer(10)(x)\n",
    "    if time:\n",
    "        x = TimeMaskLayer(10)(x)\n",
    "    x = tf.expand_dims(x, axis=-1, name='mel_stft')\n",
    "\n",
    "    x = layers.Conv2D(10, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(1, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # x = Reshape((125, 80)) (x)\n",
    "    # keras.backend.squeeze(x, axis)\n",
    "    x = layers.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True)\n",
    "                             )(x)  # [b_s, seq_len, vec_dim]\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True)\n",
    "                             )(x)  # [b_s, seq_len, vec_dim]\n",
    "\n",
    "    x_first = layers.Lambda(lambda q: q[:, -1])(x)  # [b_s, vec_dim]\n",
    "    query = layers.Dense(128)(x_first)\n",
    "\n",
    "    # dot product attention\n",
    "    att_scores = layers.Dot(axes=[1, 2])([query, x])\n",
    "    att_scores = layers.Softmax(name='attSoftmax')(att_scores)  # [b_s, seq_len]\n",
    "\n",
    "    # rescale sequence\n",
    "    att_vector = layers.Dot(axes=[1, 1])([att_scores, x])  # [b_s, vec_dim]\n",
    "    x = layers.Dropout(rate=0.3)(att_vector)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(rate=0.3)(x)\n",
    "    x = layers.Dense(32)(x)\n",
    "    x = layers.Dropout(rate=0.3)(x)\n",
    "    output = layers.Dense(N_CLASS, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(freq=True)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:54.342620Z",
     "end_time": "2023-04-24T18:14:55.734566Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(), metrics.SparseCategoricalCrossentropy()]\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='max',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=\"best_model_checkpoint_weighted\",\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', factor=0.5, patience=3,\n",
    "                                        min_lr=0.00001, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:55.717624Z",
     "end_time": "2023-04-24T18:14:55.743536Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "weights = {\n",
    "    0: 27.606189,\n",
    "    1: 27.454890,\n",
    "    2: 27.676583,\n",
    "    3: 27.420211,\n",
    "    4: 27.629614,\n",
    "    5: 27.512886,\n",
    "    6: 27.512886,\n",
    "    7: 161.997512,\n",
    "    8: 27.362605,\n",
    "    9: 1.586856,\n",
    "    10: 27.420211,\n",
    "    11: 27.397139\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:55.743536Z",
     "end_time": "2023-04-24T18:14:55.764466Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 23.4827 - sparse_categorical_accuracy: 0.1762 - sparse_categorical_crossentropy: 2.2734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 86s 657ms/step - loss: 23.4827 - sparse_categorical_accuracy: 0.1762 - sparse_categorical_crossentropy: 2.2734 - val_loss: 2.0778 - val_sparse_categorical_accuracy: 0.2354 - val_sparse_categorical_crossentropy: 2.0778 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 11.8610 - sparse_categorical_accuracy: 0.4447 - sparse_categorical_crossentropy: 1.6128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 69s 661ms/step - loss: 11.8610 - sparse_categorical_accuracy: 0.4447 - sparse_categorical_crossentropy: 1.6128 - val_loss: 1.2172 - val_sparse_categorical_accuracy: 0.5555 - val_sparse_categorical_crossentropy: 1.2172 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 7.8472 - sparse_categorical_accuracy: 0.6118 - sparse_categorical_crossentropy: 1.1755"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 67s 641ms/step - loss: 7.8472 - sparse_categorical_accuracy: 0.6118 - sparse_categorical_crossentropy: 1.1755 - val_loss: 0.8124 - val_sparse_categorical_accuracy: 0.7091 - val_sparse_categorical_crossentropy: 0.8124 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 6.1581 - sparse_categorical_accuracy: 0.6951 - sparse_categorical_crossentropy: 0.9561"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 665ms/step - loss: 6.1581 - sparse_categorical_accuracy: 0.6951 - sparse_categorical_crossentropy: 0.9561 - val_loss: 0.6151 - val_sparse_categorical_accuracy: 0.7981 - val_sparse_categorical_crossentropy: 0.6151 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 5.1559 - sparse_categorical_accuracy: 0.7473 - sparse_categorical_crossentropy: 0.8058"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 670ms/step - loss: 5.1559 - sparse_categorical_accuracy: 0.7473 - sparse_categorical_crossentropy: 0.8058 - val_loss: 0.5165 - val_sparse_categorical_accuracy: 0.8288 - val_sparse_categorical_crossentropy: 0.5165 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 4.4071 - sparse_categorical_accuracy: 0.7869 - sparse_categorical_crossentropy: 0.7008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 671ms/step - loss: 4.4071 - sparse_categorical_accuracy: 0.7869 - sparse_categorical_crossentropy: 0.7008 - val_loss: 0.4833 - val_sparse_categorical_accuracy: 0.8428 - val_sparse_categorical_crossentropy: 0.4833 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "103/103 [==============================] - 50s 465ms/step - loss: 3.8860 - sparse_categorical_accuracy: 0.8150 - sparse_categorical_crossentropy: 0.6178 - val_loss: 0.5338 - val_sparse_categorical_accuracy: 0.8316 - val_sparse_categorical_crossentropy: 0.5338 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 3.7153 - sparse_categorical_accuracy: 0.8234 - sparse_categorical_crossentropy: 0.5940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 69s 659ms/step - loss: 3.7153 - sparse_categorical_accuracy: 0.8234 - sparse_categorical_crossentropy: 0.5940 - val_loss: 0.4127 - val_sparse_categorical_accuracy: 0.8729 - val_sparse_categorical_crossentropy: 0.4127 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "103/103 [==============================] - 50s 464ms/step - loss: 3.3346 - sparse_categorical_accuracy: 0.8429 - sparse_categorical_crossentropy: 0.5374 - val_loss: 0.4097 - val_sparse_categorical_accuracy: 0.8700 - val_sparse_categorical_crossentropy: 0.4097 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 3.0160 - sparse_categorical_accuracy: 0.8603 - sparse_categorical_crossentropy: 0.4867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 667ms/step - loss: 3.0160 - sparse_categorical_accuracy: 0.8603 - sparse_categorical_crossentropy: 0.4867 - val_loss: 0.3252 - val_sparse_categorical_accuracy: 0.9047 - val_sparse_categorical_crossentropy: 0.3252 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 2.8246 - sparse_categorical_accuracy: 0.8703 - sparse_categorical_crossentropy: 0.4554"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 69s 659ms/step - loss: 2.8246 - sparse_categorical_accuracy: 0.8703 - sparse_categorical_crossentropy: 0.4554 - val_loss: 0.2606 - val_sparse_categorical_accuracy: 0.9241 - val_sparse_categorical_crossentropy: 0.2606 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "103/103 [==============================] - 50s 468ms/step - loss: 2.7175 - sparse_categorical_accuracy: 0.8772 - sparse_categorical_crossentropy: 0.4370 - val_loss: 0.2725 - val_sparse_categorical_accuracy: 0.9228 - val_sparse_categorical_crossentropy: 0.2725 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "103/103 [==============================] - 49s 466ms/step - loss: 2.5300 - sparse_categorical_accuracy: 0.8863 - sparse_categorical_crossentropy: 0.4076 - val_loss: 0.2760 - val_sparse_categorical_accuracy: 0.9218 - val_sparse_categorical_crossentropy: 0.2760 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 2.4124 - sparse_categorical_accuracy: 0.8956 - sparse_categorical_crossentropy: 0.3821\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "103/103 [==============================] - 49s 466ms/step - loss: 2.4124 - sparse_categorical_accuracy: 0.8956 - sparse_categorical_crossentropy: 0.3821 - val_loss: 0.2907 - val_sparse_categorical_accuracy: 0.9160 - val_sparse_categorical_crossentropy: 0.2907 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 2.0543 - sparse_categorical_accuracy: 0.9069 - sparse_categorical_crossentropy: 0.3331"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 68s 648ms/step - loss: 2.0543 - sparse_categorical_accuracy: 0.9069 - sparse_categorical_crossentropy: 0.3331 - val_loss: 0.2459 - val_sparse_categorical_accuracy: 0.9289 - val_sparse_categorical_crossentropy: 0.2459 - lr: 5.0000e-04\n",
      "Epoch 16/200\n",
      "103/103 [==============================] - 50s 471ms/step - loss: 1.8129 - sparse_categorical_accuracy: 0.9160 - sparse_categorical_crossentropy: 0.3041 - val_loss: 0.2766 - val_sparse_categorical_accuracy: 0.9225 - val_sparse_categorical_crossentropy: 0.2766 - lr: 5.0000e-04\n",
      "Epoch 17/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.7875 - sparse_categorical_accuracy: 0.9192 - sparse_categorical_crossentropy: 0.2909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 672ms/step - loss: 1.7875 - sparse_categorical_accuracy: 0.9192 - sparse_categorical_crossentropy: 0.2909 - val_loss: 0.2302 - val_sparse_categorical_accuracy: 0.9346 - val_sparse_categorical_crossentropy: 0.2302 - lr: 5.0000e-04\n",
      "Epoch 18/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.7262 - sparse_categorical_accuracy: 0.9184 - sparse_categorical_crossentropy: 0.2947"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 69s 659ms/step - loss: 1.7262 - sparse_categorical_accuracy: 0.9184 - sparse_categorical_crossentropy: 0.2947 - val_loss: 0.2050 - val_sparse_categorical_accuracy: 0.9433 - val_sparse_categorical_crossentropy: 0.2050 - lr: 5.0000e-04\n",
      "Epoch 19/200\n",
      "103/103 [==============================] - 50s 467ms/step - loss: 1.6711 - sparse_categorical_accuracy: 0.9254 - sparse_categorical_crossentropy: 0.2793 - val_loss: 0.2279 - val_sparse_categorical_accuracy: 0.9357 - val_sparse_categorical_crossentropy: 0.2279 - lr: 5.0000e-04\n",
      "Epoch 20/200\n",
      "103/103 [==============================] - 49s 465ms/step - loss: 1.8751 - sparse_categorical_accuracy: 0.9090 - sparse_categorical_crossentropy: 0.3576 - val_loss: 0.4897 - val_sparse_categorical_accuracy: 0.8747 - val_sparse_categorical_crossentropy: 0.4897 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.8271 - sparse_categorical_accuracy: 0.9093 - sparse_categorical_crossentropy: 0.3241\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "103/103 [==============================] - 49s 459ms/step - loss: 1.8271 - sparse_categorical_accuracy: 0.9093 - sparse_categorical_crossentropy: 0.3241 - val_loss: 0.2193 - val_sparse_categorical_accuracy: 0.9373 - val_sparse_categorical_crossentropy: 0.2193 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "103/103 [==============================] - 48s 457ms/step - loss: 1.5290 - sparse_categorical_accuracy: 0.9268 - sparse_categorical_crossentropy: 0.2637 - val_loss: 0.2075 - val_sparse_categorical_accuracy: 0.9414 - val_sparse_categorical_crossentropy: 0.2075 - lr: 2.5000e-04\n",
      "Epoch 23/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.5008 - sparse_categorical_accuracy: 0.9309 - sparse_categorical_crossentropy: 0.2565"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 68s 652ms/step - loss: 1.5008 - sparse_categorical_accuracy: 0.9309 - sparse_categorical_crossentropy: 0.2565 - val_loss: 0.2001 - val_sparse_categorical_accuracy: 0.9437 - val_sparse_categorical_crossentropy: 0.2001 - lr: 2.5000e-04\n",
      "Epoch 24/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.4846 - sparse_categorical_accuracy: 0.9324 - sparse_categorical_crossentropy: 0.2466"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 68s 643ms/step - loss: 1.4846 - sparse_categorical_accuracy: 0.9324 - sparse_categorical_crossentropy: 0.2466 - val_loss: 0.1980 - val_sparse_categorical_accuracy: 0.9452 - val_sparse_categorical_crossentropy: 0.1980 - lr: 2.5000e-04\n",
      "Epoch 25/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.3714 - sparse_categorical_accuracy: 0.9365 - sparse_categorical_crossentropy: 0.2333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 68s 651ms/step - loss: 1.3714 - sparse_categorical_accuracy: 0.9365 - sparse_categorical_crossentropy: 0.2333 - val_loss: 0.1973 - val_sparse_categorical_accuracy: 0.9453 - val_sparse_categorical_crossentropy: 0.1973 - lr: 2.5000e-04\n",
      "Epoch 26/200\n",
      "103/103 [==============================] - 49s 461ms/step - loss: 1.3885 - sparse_categorical_accuracy: 0.9340 - sparse_categorical_crossentropy: 0.2466 - val_loss: 0.2838 - val_sparse_categorical_accuracy: 0.9248 - val_sparse_categorical_crossentropy: 0.2838 - lr: 2.5000e-04\n",
      "Epoch 27/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.4482 - sparse_categorical_accuracy: 0.9300 - sparse_categorical_crossentropy: 0.2569\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "103/103 [==============================] - 49s 458ms/step - loss: 1.4482 - sparse_categorical_accuracy: 0.9300 - sparse_categorical_crossentropy: 0.2569 - val_loss: 0.2100 - val_sparse_categorical_accuracy: 0.9405 - val_sparse_categorical_crossentropy: 0.2100 - lr: 2.5000e-04\n",
      "Epoch 28/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.3395 - sparse_categorical_accuracy: 0.9392 - sparse_categorical_crossentropy: 0.2269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_checkpoint_weighted\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 68s 647ms/step - loss: 1.3395 - sparse_categorical_accuracy: 0.9392 - sparse_categorical_crossentropy: 0.2269 - val_loss: 0.1827 - val_sparse_categorical_accuracy: 0.9500 - val_sparse_categorical_crossentropy: 0.1827 - lr: 1.2500e-04\n",
      "Epoch 29/200\n",
      "103/103 [==============================] - 49s 457ms/step - loss: 1.2688 - sparse_categorical_accuracy: 0.9415 - sparse_categorical_crossentropy: 0.2144 - val_loss: 0.1880 - val_sparse_categorical_accuracy: 0.9473 - val_sparse_categorical_crossentropy: 0.1880 - lr: 1.2500e-04\n",
      "Epoch 30/200\n",
      "103/103 [==============================] - 49s 462ms/step - loss: 1.2943 - sparse_categorical_accuracy: 0.9404 - sparse_categorical_crossentropy: 0.2167 - val_loss: 0.1984 - val_sparse_categorical_accuracy: 0.9456 - val_sparse_categorical_crossentropy: 0.1984 - lr: 1.2500e-04\n",
      "Epoch 31/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.2701 - sparse_categorical_accuracy: 0.9417 - sparse_categorical_crossentropy: 0.2136\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "103/103 [==============================] - 49s 461ms/step - loss: 1.2701 - sparse_categorical_accuracy: 0.9417 - sparse_categorical_crossentropy: 0.2136 - val_loss: 0.1870 - val_sparse_categorical_accuracy: 0.9479 - val_sparse_categorical_crossentropy: 0.1870 - lr: 1.2500e-04\n",
      "Epoch 32/200\n",
      "103/103 [==============================] - 51s 481ms/step - loss: 1.2102 - sparse_categorical_accuracy: 0.9446 - sparse_categorical_crossentropy: 0.2043 - val_loss: 0.1906 - val_sparse_categorical_accuracy: 0.9476 - val_sparse_categorical_crossentropy: 0.1906 - lr: 6.2500e-05\n",
      "Epoch 33/200\n",
      "103/103 [==============================] - 52s 492ms/step - loss: 1.2643 - sparse_categorical_accuracy: 0.9441 - sparse_categorical_crossentropy: 0.2073 - val_loss: 0.1868 - val_sparse_categorical_accuracy: 0.9476 - val_sparse_categorical_crossentropy: 0.1868 - lr: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    class_weight=weights\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:55.762473Z",
     "end_time": "2023-04-24T18:47:38.800250Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 53s 168ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:47:38.803240Z",
     "end_time": "2023-04-24T18:48:32.429096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "predict_class = np.array(list(label_names[i] for i in np.argmax(predictions, axis=1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:48:32.433080Z",
     "end_time": "2023-04-24T18:48:32.646148Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for i, (root, dir, fname) in enumerate(os.walk(os.path.join(TEST_PATH, 'audio'))):\n",
    "    files = np.array(fname)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:48:32.652128Z",
     "end_time": "2023-04-24T18:48:33.113685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.vstack([files, predict_class]).T, columns=[\"fname\", \"label\"])\n",
    "submission.to_csv(\"submission_reweighted.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:48:33.116674Z",
     "end_time": "2023-04-24T18:48:33.395738Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
