{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:16.411740Z",
     "end_time": "2023-04-23T17:28:19.023203Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import losses, optimizers, metrics, callbacks\n",
    "\n",
    "import SpeechModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:19.024203Z",
     "end_time": "2023-04-23T17:28:19.615203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:19.613202Z",
     "end_time": "2023-04-23T17:28:19.657202Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "N_CLASS = 12\n",
    "MAX_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:19.629203Z",
     "end_time": "2023-04-23T17:28:19.664202Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45586 files belonging to 12 classes.\n",
      "Found 6513 files belonging to 12 classes.\n",
      "Found 13024 files belonging to 12 classes.\n",
      "label names: ['down' 'go' 'left' 'no' 'off' 'on' 'right' 'silence' 'stop' 'unknown'\n",
      " 'up' 'yes']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/train\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/val\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/test\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print(\"label names:\", label_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:19.646202Z",
     "end_time": "2023-04-23T17:28:24.972198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(squeeze, tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:24.975198Z",
     "end_time": "2023-04-23T17:28:25.037198Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model from article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " normalized_spectrogram_model (  (None, None, 80)    0           ['input[0][0]']                  \n",
      " Functional)                                                                                      \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, None, 80, 1)  0           ['normalized_spectrogram_model[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, None, 80, 10  60          ['tf.expand_dims[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, None, 80, 10  40         ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, None, 80, 1)  51          ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, None, 80, 1)  4          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " squeeze_last_dim (Lambda)      (None, None, 80)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, None, 128)    74240       ['squeeze_last_dim[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, None, 128)   98816       ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 128)          0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, None)         0           ['dense[0][0]',                  \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " attSoftmax (Softmax)           (None, None)         0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 128)          0           ['attSoftmax[0][0]',             \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 12)           396         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 200,455\n",
      "Trainable params: 200,433\n",
      "Non-trainable params: 22\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SpeechModels.AttRNNSpeechModel(N_CLASS, samplingrate = 16000, inputLength = None)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:25.043198Z",
     "end_time": "2023-04-23T17:28:26.248801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:26.214801Z",
     "end_time": "2023-04-23T17:28:26.249802Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(), metrics.SparseCategoricalCrossentropy()]\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='max',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', factor=0.5, patience=3, min_lr=0.00001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:26.242801Z",
     "end_time": "2023-04-23T17:29:14.333890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "90/90 [==============================] - 30s 191ms/step - loss: 1.5605 - sparse_categorical_accuracy: 0.6241 - sparse_categorical_crossentropy: 1.5605 - val_loss: 1.5286 - val_sparse_categorical_accuracy: 0.6301 - val_sparse_categorical_crossentropy: 1.5286 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 1.0886 - sparse_categorical_accuracy: 0.6837 - sparse_categorical_crossentropy: 1.0886 - val_loss: 1.0332 - val_sparse_categorical_accuracy: 0.6843 - val_sparse_categorical_crossentropy: 1.0332 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=2,\n",
    "    validation_data=val_ds,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Training will be repeated 5 times with different weights initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"article_net\"\n",
    "if os.path.exists(EXPERIMENT_NAME):\n",
    "    shutil.rmtree(EXPERIMENT_NAME)\n",
    "    os.mkdir(EXPERIMENT_NAME)\n",
    "else:\n",
    "    os.mkdir(EXPERIMENT_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T17:29:13.742891Z",
     "end_time": "2023-04-23T17:29:14.333890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "90/90 [==============================] - 24s 194ms/step - loss: 1.1919 - sparse_categorical_accuracy: 0.6710 - sparse_categorical_crossentropy: 1.1919 - val_loss: 0.8478 - val_sparse_categorical_accuracy: 0.7339 - val_sparse_categorical_crossentropy: 0.8478 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.4384 - sparse_categorical_accuracy: 0.8688 - sparse_categorical_crossentropy: 0.4384 - val_loss: 0.4577 - val_sparse_categorical_accuracy: 0.8629 - val_sparse_categorical_crossentropy: 0.4577 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.2820 - sparse_categorical_accuracy: 0.9165 - sparse_categorical_crossentropy: 0.2820 - val_loss: 0.3281 - val_sparse_categorical_accuracy: 0.8931 - val_sparse_categorical_crossentropy: 0.3281 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 17s 175ms/step - loss: 0.2087 - sparse_categorical_accuracy: 0.9377 - sparse_categorical_crossentropy: 0.2087 - val_loss: 0.2216 - val_sparse_categorical_accuracy: 0.9329 - val_sparse_categorical_crossentropy: 0.2216 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.1509 - sparse_categorical_accuracy: 0.9548 - sparse_categorical_crossentropy: 0.1509 - val_loss: 0.1888 - val_sparse_categorical_accuracy: 0.9461 - val_sparse_categorical_crossentropy: 0.1888 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.1279 - sparse_categorical_accuracy: 0.9624 - sparse_categorical_crossentropy: 0.1279 - val_loss: 0.1628 - val_sparse_categorical_accuracy: 0.9539 - val_sparse_categorical_crossentropy: 0.1628 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.1070 - sparse_categorical_accuracy: 0.9684 - sparse_categorical_crossentropy: 0.1070 - val_loss: 0.1590 - val_sparse_categorical_accuracy: 0.9532 - val_sparse_categorical_crossentropy: 0.1590 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.0884 - sparse_categorical_accuracy: 0.9737 - sparse_categorical_crossentropy: 0.0884 - val_loss: 0.1706 - val_sparse_categorical_accuracy: 0.9495 - val_sparse_categorical_crossentropy: 0.1706 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0851 - sparse_categorical_accuracy: 0.9739 - sparse_categorical_crossentropy: 0.0851\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.0851 - sparse_categorical_accuracy: 0.9739 - sparse_categorical_crossentropy: 0.0851 - val_loss: 0.2053 - val_sparse_categorical_accuracy: 0.9409 - val_sparse_categorical_crossentropy: 0.2053 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 17s 176ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9831 - sparse_categorical_crossentropy: 0.0590 - val_loss: 0.1395 - val_sparse_categorical_accuracy: 0.9636 - val_sparse_categorical_crossentropy: 0.1395 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9888 - sparse_categorical_crossentropy: 0.0403 - val_loss: 0.1325 - val_sparse_categorical_accuracy: 0.9667 - val_sparse_categorical_crossentropy: 0.1325 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0314 - sparse_categorical_accuracy: 0.9916 - sparse_categorical_crossentropy: 0.0314 - val_loss: 0.1453 - val_sparse_categorical_accuracy: 0.9650 - val_sparse_categorical_crossentropy: 0.1453 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.9923 - sparse_categorical_crossentropy: 0.0285 - val_loss: 0.1600 - val_sparse_categorical_accuracy: 0.9642 - val_sparse_categorical_crossentropy: 0.1600 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0620 - sparse_categorical_accuracy: 0.9816 - sparse_categorical_crossentropy: 0.0620\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0620 - sparse_categorical_accuracy: 0.9816 - sparse_categorical_crossentropy: 0.0620 - val_loss: 0.1664 - val_sparse_categorical_accuracy: 0.9587 - val_sparse_categorical_crossentropy: 0.1664 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0304 - sparse_categorical_accuracy: 0.9914 - sparse_categorical_crossentropy: 0.0304 - val_loss: 0.1509 - val_sparse_categorical_accuracy: 0.9651 - val_sparse_categorical_crossentropy: 0.1509 - lr: 0.0025\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 18s 190ms/step - loss: 0.0221 - sparse_categorical_accuracy: 0.9940 - sparse_categorical_crossentropy: 0.0221 - val_loss: 0.1628 - val_sparse_categorical_accuracy: 0.9635 - val_sparse_categorical_crossentropy: 0.1628 - lr: 0.0025\n",
      "26/26 [==============================] - 3s 101ms/step - loss: 0.1457 - sparse_categorical_accuracy: 0.9650 - sparse_categorical_crossentropy: 0.1457\n",
      "26/26 [==============================] - 4s 96ms/step\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 24s 198ms/step - loss: 1.3093 - sparse_categorical_accuracy: 0.6501 - sparse_categorical_crossentropy: 1.3093 - val_loss: 1.4670 - val_sparse_categorical_accuracy: 0.5813 - val_sparse_categorical_crossentropy: 1.4670 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 18s 189ms/step - loss: 0.5535 - sparse_categorical_accuracy: 0.8286 - sparse_categorical_crossentropy: 0.5535 - val_loss: 1.3663 - val_sparse_categorical_accuracy: 0.6513 - val_sparse_categorical_crossentropy: 1.3663 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 18s 194ms/step - loss: 0.3047 - sparse_categorical_accuracy: 0.9060 - sparse_categorical_crossentropy: 0.3047 - val_loss: 0.8041 - val_sparse_categorical_accuracy: 0.7791 - val_sparse_categorical_crossentropy: 0.8041 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 18s 190ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9392 - sparse_categorical_crossentropy: 0.1969 - val_loss: 0.6632 - val_sparse_categorical_accuracy: 0.8139 - val_sparse_categorical_crossentropy: 0.6632 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 18s 192ms/step - loss: 0.1765 - sparse_categorical_accuracy: 0.9461 - sparse_categorical_crossentropy: 0.1765 - val_loss: 0.5187 - val_sparse_categorical_accuracy: 0.8489 - val_sparse_categorical_crossentropy: 0.5187 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 18s 188ms/step - loss: 0.1339 - sparse_categorical_accuracy: 0.9592 - sparse_categorical_crossentropy: 0.1339 - val_loss: 0.2138 - val_sparse_categorical_accuracy: 0.9378 - val_sparse_categorical_crossentropy: 0.2138 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 18s 188ms/step - loss: 0.1250 - sparse_categorical_accuracy: 0.9623 - sparse_categorical_crossentropy: 0.1250 - val_loss: 0.1550 - val_sparse_categorical_accuracy: 0.9546 - val_sparse_categorical_crossentropy: 0.1550 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 19s 190ms/step - loss: 0.0976 - sparse_categorical_accuracy: 0.9703 - sparse_categorical_crossentropy: 0.0976 - val_loss: 0.2915 - val_sparse_categorical_accuracy: 0.9159 - val_sparse_categorical_crossentropy: 0.2915 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 18s 189ms/step - loss: 0.0769 - sparse_categorical_accuracy: 0.9771 - sparse_categorical_crossentropy: 0.0769 - val_loss: 0.1352 - val_sparse_categorical_accuracy: 0.9596 - val_sparse_categorical_crossentropy: 0.1352 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0686 - sparse_categorical_accuracy: 0.9792 - sparse_categorical_crossentropy: 0.0686 - val_loss: 0.1797 - val_sparse_categorical_accuracy: 0.9506 - val_sparse_categorical_crossentropy: 0.1797 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 18s 187ms/step - loss: 0.0671 - sparse_categorical_accuracy: 0.9800 - sparse_categorical_crossentropy: 0.0671 - val_loss: 0.1928 - val_sparse_categorical_accuracy: 0.9483 - val_sparse_categorical_crossentropy: 0.1928 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1432 - sparse_categorical_accuracy: 0.9558 - sparse_categorical_crossentropy: 0.1432\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 18s 193ms/step - loss: 0.1431 - sparse_categorical_accuracy: 0.9558 - sparse_categorical_crossentropy: 0.1431 - val_loss: 0.1789 - val_sparse_categorical_accuracy: 0.9467 - val_sparse_categorical_crossentropy: 0.1789 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0719 - sparse_categorical_accuracy: 0.9782 - sparse_categorical_crossentropy: 0.0719 - val_loss: 0.1200 - val_sparse_categorical_accuracy: 0.9682 - val_sparse_categorical_crossentropy: 0.1200 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9883 - sparse_categorical_crossentropy: 0.0415 - val_loss: 0.1289 - val_sparse_categorical_accuracy: 0.9681 - val_sparse_categorical_crossentropy: 0.1289 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9915 - sparse_categorical_crossentropy: 0.0316 - val_loss: 0.1279 - val_sparse_categorical_accuracy: 0.9690 - val_sparse_categorical_crossentropy: 0.1279 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 18s 184ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9929 - sparse_categorical_crossentropy: 0.0264 - val_loss: 0.1367 - val_sparse_categorical_accuracy: 0.9682 - val_sparse_categorical_crossentropy: 0.1367 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9938 - sparse_categorical_crossentropy: 0.0231 - val_loss: 0.1473 - val_sparse_categorical_accuracy: 0.9681 - val_sparse_categorical_crossentropy: 0.1473 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0269 - sparse_categorical_accuracy: 0.9925 - sparse_categorical_crossentropy: 0.0269\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9925 - sparse_categorical_crossentropy: 0.0269 - val_loss: 0.1554 - val_sparse_categorical_accuracy: 0.9661 - val_sparse_categorical_crossentropy: 0.1554 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - 18s 193ms/step - loss: 0.0236 - sparse_categorical_accuracy: 0.9932 - sparse_categorical_crossentropy: 0.0236 - val_loss: 0.1564 - val_sparse_categorical_accuracy: 0.9682 - val_sparse_categorical_crossentropy: 0.1564 - lr: 0.0025\n",
      "Epoch 20/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9950 - sparse_categorical_crossentropy: 0.0185 - val_loss: 0.1501 - val_sparse_categorical_accuracy: 0.9707 - val_sparse_categorical_crossentropy: 0.1501 - lr: 0.0025\n",
      "Epoch 21/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9955 - sparse_categorical_crossentropy: 0.0165 - val_loss: 0.1621 - val_sparse_categorical_accuracy: 0.9708 - val_sparse_categorical_crossentropy: 0.1621 - lr: 0.0025\n",
      "Epoch 22/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9956 - sparse_categorical_crossentropy: 0.0155 - val_loss: 0.1691 - val_sparse_categorical_accuracy: 0.9698 - val_sparse_categorical_crossentropy: 0.1691 - lr: 0.0025\n",
      "Epoch 23/200\n",
      "90/90 [==============================] - 18s 189ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9959 - sparse_categorical_crossentropy: 0.0146 - val_loss: 0.1802 - val_sparse_categorical_accuracy: 0.9684 - val_sparse_categorical_crossentropy: 0.1802 - lr: 0.0025\n",
      "Epoch 24/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0140 - sparse_categorical_accuracy: 0.9960 - sparse_categorical_crossentropy: 0.0140\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0140 - sparse_categorical_accuracy: 0.9960 - sparse_categorical_crossentropy: 0.0140 - val_loss: 0.1837 - val_sparse_categorical_accuracy: 0.9681 - val_sparse_categorical_crossentropy: 0.1837 - lr: 0.0025\n",
      "Epoch 25/200\n",
      "90/90 [==============================] - 17s 172ms/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9962 - sparse_categorical_crossentropy: 0.0131 - val_loss: 0.1898 - val_sparse_categorical_accuracy: 0.9676 - val_sparse_categorical_crossentropy: 0.1898 - lr: 0.0012\n",
      "Epoch 26/200\n",
      "90/90 [==============================] - 18s 186ms/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9965 - sparse_categorical_crossentropy: 0.0119 - val_loss: 0.1930 - val_sparse_categorical_accuracy: 0.9668 - val_sparse_categorical_crossentropy: 0.1930 - lr: 0.0012\n",
      "26/26 [==============================] - 3s 111ms/step - loss: 0.1929 - sparse_categorical_accuracy: 0.9643 - sparse_categorical_crossentropy: 0.1929\n",
      "26/26 [==============================] - 5s 117ms/step\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 23s 190ms/step - loss: 0.9550 - sparse_categorical_accuracy: 0.7212 - sparse_categorical_crossentropy: 0.9550 - val_loss: 1.1133 - val_sparse_categorical_accuracy: 0.6925 - val_sparse_categorical_crossentropy: 1.1133 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.4448 - sparse_categorical_accuracy: 0.8616 - sparse_categorical_crossentropy: 0.4448 - val_loss: 0.3818 - val_sparse_categorical_accuracy: 0.8741 - val_sparse_categorical_crossentropy: 0.3818 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.2752 - sparse_categorical_accuracy: 0.9141 - sparse_categorical_crossentropy: 0.2752 - val_loss: 0.2889 - val_sparse_categorical_accuracy: 0.9099 - val_sparse_categorical_crossentropy: 0.2889 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 17s 175ms/step - loss: 0.2120 - sparse_categorical_accuracy: 0.9339 - sparse_categorical_crossentropy: 0.2120 - val_loss: 0.2266 - val_sparse_categorical_accuracy: 0.9300 - val_sparse_categorical_crossentropy: 0.2266 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.1567 - sparse_categorical_accuracy: 0.9512 - sparse_categorical_crossentropy: 0.1567 - val_loss: 0.2312 - val_sparse_categorical_accuracy: 0.9315 - val_sparse_categorical_crossentropy: 0.2312 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.1578 - sparse_categorical_accuracy: 0.9507 - sparse_categorical_crossentropy: 0.1578 - val_loss: 0.2557 - val_sparse_categorical_accuracy: 0.9261 - val_sparse_categorical_crossentropy: 0.2557 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.1468 - sparse_categorical_accuracy: 0.9535 - sparse_categorical_crossentropy: 0.1468 - val_loss: 0.1950 - val_sparse_categorical_accuracy: 0.9413 - val_sparse_categorical_crossentropy: 0.1950 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.1196 - sparse_categorical_accuracy: 0.9639 - sparse_categorical_crossentropy: 0.1196 - val_loss: 0.2069 - val_sparse_categorical_accuracy: 0.9413 - val_sparse_categorical_crossentropy: 0.2069 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.1213 - sparse_categorical_accuracy: 0.9625 - sparse_categorical_crossentropy: 0.1213 - val_loss: 0.2271 - val_sparse_categorical_accuracy: 0.9306 - val_sparse_categorical_crossentropy: 0.2271 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.1180 - sparse_categorical_accuracy: 0.9642 - sparse_categorical_crossentropy: 0.1180 - val_loss: 0.1552 - val_sparse_categorical_accuracy: 0.9572 - val_sparse_categorical_crossentropy: 0.1552 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0759 - sparse_categorical_accuracy: 0.9774 - sparse_categorical_crossentropy: 0.0759 - val_loss: 0.1445 - val_sparse_categorical_accuracy: 0.9604 - val_sparse_categorical_crossentropy: 0.1445 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0673 - sparse_categorical_accuracy: 0.9800 - sparse_categorical_crossentropy: 0.0673 - val_loss: 0.1956 - val_sparse_categorical_accuracy: 0.9501 - val_sparse_categorical_crossentropy: 0.1956 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0724 - sparse_categorical_accuracy: 0.9780 - sparse_categorical_crossentropy: 0.0724 - val_loss: 0.1736 - val_sparse_categorical_accuracy: 0.9558 - val_sparse_categorical_crossentropy: 0.1736 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0823 - sparse_categorical_accuracy: 0.9751 - sparse_categorical_crossentropy: 0.0823\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0823 - sparse_categorical_accuracy: 0.9751 - sparse_categorical_crossentropy: 0.0823 - val_loss: 0.1810 - val_sparse_categorical_accuracy: 0.9496 - val_sparse_categorical_crossentropy: 0.1810 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9842 - sparse_categorical_crossentropy: 0.0514 - val_loss: 0.1264 - val_sparse_categorical_accuracy: 0.9662 - val_sparse_categorical_crossentropy: 0.1264 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.9896 - sparse_categorical_crossentropy: 0.0367 - val_loss: 0.1432 - val_sparse_categorical_accuracy: 0.9648 - val_sparse_categorical_crossentropy: 0.1432 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9926 - sparse_categorical_crossentropy: 0.0269 - val_loss: 0.1438 - val_sparse_categorical_accuracy: 0.9670 - val_sparse_categorical_crossentropy: 0.1438 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.0225 - sparse_categorical_accuracy: 0.9938 - sparse_categorical_crossentropy: 0.0225 - val_loss: 0.1432 - val_sparse_categorical_accuracy: 0.9659 - val_sparse_categorical_crossentropy: 0.1432 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.0213 - sparse_categorical_accuracy: 0.9940 - sparse_categorical_crossentropy: 0.0213 - val_loss: 0.1636 - val_sparse_categorical_accuracy: 0.9664 - val_sparse_categorical_crossentropy: 0.1636 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0184 - sparse_categorical_accuracy: 0.9947 - sparse_categorical_crossentropy: 0.0184\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9947 - sparse_categorical_crossentropy: 0.0184 - val_loss: 0.1754 - val_sparse_categorical_accuracy: 0.9651 - val_sparse_categorical_crossentropy: 0.1754 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy: 0.0162 - val_loss: 0.1673 - val_sparse_categorical_accuracy: 0.9676 - val_sparse_categorical_crossentropy: 0.1673 - lr: 0.0025\n",
      "Epoch 22/200\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9955 - sparse_categorical_crossentropy: 0.0158 - val_loss: 0.1822 - val_sparse_categorical_accuracy: 0.9659 - val_sparse_categorical_crossentropy: 0.1822 - lr: 0.0025\n",
      "Epoch 23/200\n",
      "90/90 [==============================] - 17s 176ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9958 - sparse_categorical_crossentropy: 0.0146 - val_loss: 0.1918 - val_sparse_categorical_accuracy: 0.9639 - val_sparse_categorical_crossentropy: 0.1918 - lr: 0.0025\n",
      "Epoch 24/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0138 - sparse_categorical_accuracy: 0.9961 - sparse_categorical_crossentropy: 0.0138\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9961 - sparse_categorical_crossentropy: 0.0138 - val_loss: 0.1891 - val_sparse_categorical_accuracy: 0.9674 - val_sparse_categorical_crossentropy: 0.1891 - lr: 0.0025\n",
      "Epoch 25/200\n",
      "90/90 [==============================] - 17s 173ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9965 - sparse_categorical_crossentropy: 0.0126 - val_loss: 0.1926 - val_sparse_categorical_accuracy: 0.9671 - val_sparse_categorical_crossentropy: 0.1926 - lr: 0.0012\n",
      "Epoch 26/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9967 - sparse_categorical_crossentropy: 0.0117 - val_loss: 0.1960 - val_sparse_categorical_accuracy: 0.9671 - val_sparse_categorical_crossentropy: 0.1960 - lr: 0.0012\n",
      "26/26 [==============================] - 3s 110ms/step - loss: 0.1934 - sparse_categorical_accuracy: 0.9627 - sparse_categorical_crossentropy: 0.1934\n",
      "26/26 [==============================] - 4s 99ms/step\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 23s 187ms/step - loss: 0.9078 - sparse_categorical_accuracy: 0.7356 - sparse_categorical_crossentropy: 0.9078 - val_loss: 1.9197 - val_sparse_categorical_accuracy: 0.5828 - val_sparse_categorical_crossentropy: 1.9197 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.3751 - sparse_categorical_accuracy: 0.8832 - sparse_categorical_crossentropy: 0.3751 - val_loss: 0.9419 - val_sparse_categorical_accuracy: 0.7112 - val_sparse_categorical_crossentropy: 0.9419 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.2522 - sparse_categorical_accuracy: 0.9206 - sparse_categorical_crossentropy: 0.2522 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9071 - val_sparse_categorical_crossentropy: 0.2874 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.2247 - sparse_categorical_accuracy: 0.9298 - sparse_categorical_crossentropy: 0.2247 - val_loss: 0.3418 - val_sparse_categorical_accuracy: 0.9059 - val_sparse_categorical_crossentropy: 0.3418 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 17s 175ms/step - loss: 0.2147 - sparse_categorical_accuracy: 0.9333 - sparse_categorical_crossentropy: 0.2147 - val_loss: 0.2045 - val_sparse_categorical_accuracy: 0.9340 - val_sparse_categorical_crossentropy: 0.2045 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.1308 - sparse_categorical_accuracy: 0.9594 - sparse_categorical_crossentropy: 0.1308 - val_loss: 0.1499 - val_sparse_categorical_accuracy: 0.9553 - val_sparse_categorical_crossentropy: 0.1499 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 17s 173ms/step - loss: 0.1117 - sparse_categorical_accuracy: 0.9651 - sparse_categorical_crossentropy: 0.1117 - val_loss: 0.1585 - val_sparse_categorical_accuracy: 0.9536 - val_sparse_categorical_crossentropy: 0.1585 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 17s 175ms/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9718 - sparse_categorical_crossentropy: 0.0919 - val_loss: 0.1410 - val_sparse_categorical_accuracy: 0.9585 - val_sparse_categorical_crossentropy: 0.1410 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.1136 - sparse_categorical_accuracy: 0.9648 - sparse_categorical_crossentropy: 0.1136 - val_loss: 0.1600 - val_sparse_categorical_accuracy: 0.9536 - val_sparse_categorical_crossentropy: 0.1600 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9712 - sparse_categorical_crossentropy: 0.0931 - val_loss: 0.1439 - val_sparse_categorical_accuracy: 0.9590 - val_sparse_categorical_crossentropy: 0.1439 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 17s 173ms/step - loss: 0.0683 - sparse_categorical_accuracy: 0.9796 - sparse_categorical_crossentropy: 0.0683 - val_loss: 0.1309 - val_sparse_categorical_accuracy: 0.9639 - val_sparse_categorical_crossentropy: 0.1309 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 17s 176ms/step - loss: 0.0658 - sparse_categorical_accuracy: 0.9804 - sparse_categorical_crossentropy: 0.0658 - val_loss: 0.1348 - val_sparse_categorical_accuracy: 0.9610 - val_sparse_categorical_crossentropy: 0.1348 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.0648 - sparse_categorical_accuracy: 0.9807 - sparse_categorical_crossentropy: 0.0648 - val_loss: 0.1751 - val_sparse_categorical_accuracy: 0.9535 - val_sparse_categorical_crossentropy: 0.1751 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.0898 - sparse_categorical_accuracy: 0.9725 - sparse_categorical_crossentropy: 0.0898 - val_loss: 0.1246 - val_sparse_categorical_accuracy: 0.9645 - val_sparse_categorical_crossentropy: 0.1246 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9821 - sparse_categorical_crossentropy: 0.0584 - val_loss: 0.1321 - val_sparse_categorical_accuracy: 0.9635 - val_sparse_categorical_crossentropy: 0.1321 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 17s 178ms/step - loss: 0.0484 - sparse_categorical_accuracy: 0.9854 - sparse_categorical_crossentropy: 0.0484 - val_loss: 0.1632 - val_sparse_categorical_accuracy: 0.9596 - val_sparse_categorical_crossentropy: 0.1632 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0504 - sparse_categorical_accuracy: 0.9845 - sparse_categorical_crossentropy: 0.0504\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9845 - sparse_categorical_crossentropy: 0.0504 - val_loss: 0.1491 - val_sparse_categorical_accuracy: 0.9630 - val_sparse_categorical_crossentropy: 0.1491 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "90/90 [==============================] - 17s 177ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9895 - sparse_categorical_crossentropy: 0.0359 - val_loss: 0.1280 - val_sparse_categorical_accuracy: 0.9682 - val_sparse_categorical_crossentropy: 0.1280 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0243 - sparse_categorical_accuracy: 0.9934 - sparse_categorical_crossentropy: 0.0243 - val_loss: 0.1256 - val_sparse_categorical_accuracy: 0.9705 - val_sparse_categorical_crossentropy: 0.1256 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9944 - sparse_categorical_crossentropy: 0.0202 - val_loss: 0.1421 - val_sparse_categorical_accuracy: 0.9688 - val_sparse_categorical_crossentropy: 0.1421 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "90/90 [==============================] - 17s 174ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9948 - sparse_categorical_crossentropy: 0.0178 - val_loss: 0.1504 - val_sparse_categorical_accuracy: 0.9693 - val_sparse_categorical_crossentropy: 0.1504 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0202 - sparse_categorical_accuracy: 0.9942 - sparse_categorical_crossentropy: 0.0202\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9942 - sparse_categorical_crossentropy: 0.0201 - val_loss: 0.1530 - val_sparse_categorical_accuracy: 0.9688 - val_sparse_categorical_crossentropy: 0.1530 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "60/90 [===================>..........] - ETA: 5s - loss: 0.0168 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy: 0.0168"
     ]
    }
   ],
   "source": [
    "TRAINING_SEEDS = list(range(5))\n",
    "results = []\n",
    "for seed in TRAINING_SEEDS:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    model = SpeechModels.AttRNNSpeechModel(N_CLASS, samplingrate = 16000, inputLength = None)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "        loss=losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[metrics.SparseCategoricalAccuracy(), metrics.SparseCategoricalCrossentropy()]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(EXPERIMENT_NAME, f\"history_{seed}.pkl\"), \"wb\") as file:\n",
    "        pickle.dump(history.history, file)\n",
    "\n",
    "    eval_results = model.evaluate(test_ds)\n",
    "\n",
    "    predictions = model.predict(test_ds)\n",
    "    with open(os.path.join(EXPERIMENT_NAME, f\"predictions_{seed}.pkl\"), \"wb\") as file:\n",
    "        pickle.dump(predictions, file)\n",
    "\n",
    "    results += [{\n",
    "        'seed': seed,\n",
    "        'results': dict(zip(model.metrics_names, eval_results))\n",
    "    }]\n",
    "    gc.collect()\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results = pd.concat([results.drop([\"results\"], axis=1), results[\"results\"].apply(pd.Series)], axis=1)\n",
    "results.to_csv(os.path.join(EXPERIMENT_NAME, 'results.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T17:29:13.782891Z",
     "end_time": "2023-04-23T18:04:37.253766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T17:28:10.184806Z",
     "end_time": "2023-04-23T18:04:37.267766Z"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
