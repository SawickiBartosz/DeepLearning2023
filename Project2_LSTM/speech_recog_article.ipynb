{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:10.566508Z",
     "end_time": "2023-04-22T23:55:15.048231Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import losses, optimizers, metrics, callbacks\n",
    "\n",
    "import SpeechModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:15.050231Z",
     "end_time": "2023-04-22T23:55:15.656230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:15.656230Z",
     "end_time": "2023-04-22T23:55:15.702237Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "N_CLASS = 12\n",
    "MAX_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:15.673238Z",
     "end_time": "2023-04-22T23:55:15.710237Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45586 files belonging to 12 classes.\n",
      "Found 6513 files belonging to 12 classes.\n",
      "Found 13024 files belonging to 12 classes.\n",
      "label names: ['down' 'go' 'left' 'no' 'off' 'on' 'right' 'silence' 'stop' 'unknown'\n",
      " 'up' 'yes']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/train\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/val\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/test\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000)\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print(\"label names:\", label_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:15.691237Z",
     "end_time": "2023-04-22T23:55:21.781819Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(squeeze, tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:21.593306Z",
     "end_time": "2023-04-22T23:55:21.781819Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model from article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " normalized_spectrogram_model (  (None, None, 80)    0           ['input[0][0]']                  \n",
      " Functional)                                                                                      \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, None, 80, 1)  0           ['normalized_spectrogram_model[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, None, 80, 10  60          ['tf.expand_dims[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, None, 80, 10  40         ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, None, 80, 1)  51          ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, None, 80, 1)  4          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " squeeze_last_dim (Lambda)      (None, None, 80)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, None, 128)    74240       ['squeeze_last_dim[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, None, 128)   98816       ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 128)          0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, None)         0           ['dense[0][0]',                  \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " attSoftmax (Softmax)           (None, None)         0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 128)          0           ['attSoftmax[0][0]',             \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 12)           396         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 200,455\n",
      "Trainable params: 200,433\n",
      "Non-trainable params: 22\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SpeechModels.AttRNNSpeechModel(N_CLASS, samplingrate = 16000, inputLength = None)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:21.652307Z",
     "end_time": "2023-04-22T23:55:22.963817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:22.940817Z",
     "end_time": "2023-04-22T23:55:22.967818Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(), metrics.SparseCategoricalCrossentropy()]\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='max',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', factor=0.5, patience=3, min_lr=0.00001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-22T23:55:22.967818Z",
     "end_time": "2023-04-22T23:56:10.772953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "90/90 [==============================] - 31s 195ms/step - loss: 1.5606 - sparse_categorical_accuracy: 0.6241 - sparse_categorical_crossentropy: 1.5606 - val_loss: 1.5222 - val_sparse_categorical_accuracy: 0.6301 - val_sparse_categorical_crossentropy: 1.5222 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 1.0955 - sparse_categorical_accuracy: 0.6818 - sparse_categorical_crossentropy: 1.0955 - val_loss: 0.9416 - val_sparse_categorical_accuracy: 0.7069 - val_sparse_categorical_crossentropy: 0.9416 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=2,\n",
    "    validation_data=val_ds,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Training will be repeated 5 times with different weights initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"article_net\"\n",
    "if os.path.exists(EXPERIMENT_NAME):\n",
    "    shutil.rmtree(EXPERIMENT_NAME)\n",
    "    os.mkdir(EXPERIMENT_NAME)\n",
    "else:\n",
    "    os.mkdir(EXPERIMENT_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T23:56:10.774953Z",
     "end_time": "2023-04-22T23:56:10.788953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "90/90 [==============================] - 24s 202ms/step - loss: 1.1906 - sparse_categorical_accuracy: 0.6708 - sparse_categorical_crossentropy: 1.1906 - val_loss: 1.5515 - val_sparse_categorical_accuracy: 0.6301 - val_sparse_categorical_crossentropy: 1.5515 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.4070 - sparse_categorical_accuracy: 0.8729 - sparse_categorical_crossentropy: 0.4070 - val_loss: 1.5573 - val_sparse_categorical_accuracy: 0.6304 - val_sparse_categorical_crossentropy: 1.5573 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 17s 183ms/step - loss: 0.2303 - sparse_categorical_accuracy: 0.9278 - sparse_categorical_crossentropy: 0.2303 - val_loss: 1.5943 - val_sparse_categorical_accuracy: 0.6180 - val_sparse_categorical_crossentropy: 1.5943 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.1782 - sparse_categorical_accuracy: 0.9450 - sparse_categorical_crossentropy: 0.1782 - val_loss: 1.0340 - val_sparse_categorical_accuracy: 0.7167 - val_sparse_categorical_crossentropy: 1.0340 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 18s 186ms/step - loss: 0.1326 - sparse_categorical_accuracy: 0.9603 - sparse_categorical_crossentropy: 0.1326 - val_loss: 0.3767 - val_sparse_categorical_accuracy: 0.8882 - val_sparse_categorical_crossentropy: 0.3767 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 18s 187ms/step - loss: 0.1139 - sparse_categorical_accuracy: 0.9665 - sparse_categorical_crossentropy: 0.1139 - val_loss: 0.2327 - val_sparse_categorical_accuracy: 0.9300 - val_sparse_categorical_crossentropy: 0.2327 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 18s 186ms/step - loss: 0.0866 - sparse_categorical_accuracy: 0.9741 - sparse_categorical_crossentropy: 0.0866 - val_loss: 0.2990 - val_sparse_categorical_accuracy: 0.9139 - val_sparse_categorical_crossentropy: 0.2990 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 18s 186ms/step - loss: 0.0778 - sparse_categorical_accuracy: 0.9759 - sparse_categorical_crossentropy: 0.0778 - val_loss: 0.2517 - val_sparse_categorical_accuracy: 0.9268 - val_sparse_categorical_crossentropy: 0.2517 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0720 - sparse_categorical_accuracy: 0.9781 - sparse_categorical_crossentropy: 0.0720\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.0721 - sparse_categorical_accuracy: 0.9780 - sparse_categorical_crossentropy: 0.0721 - val_loss: 0.4818 - val_sparse_categorical_accuracy: 0.8653 - val_sparse_categorical_crossentropy: 0.4818 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0857 - sparse_categorical_accuracy: 0.9746 - sparse_categorical_crossentropy: 0.0857 - val_loss: 0.1397 - val_sparse_categorical_accuracy: 0.9619 - val_sparse_categorical_crossentropy: 0.1397 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.0409 - sparse_categorical_accuracy: 0.9879 - sparse_categorical_crossentropy: 0.0409 - val_loss: 0.1493 - val_sparse_categorical_accuracy: 0.9610 - val_sparse_categorical_crossentropy: 0.1493 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 18s 184ms/step - loss: 0.0300 - sparse_categorical_accuracy: 0.9919 - sparse_categorical_crossentropy: 0.0300 - val_loss: 0.1697 - val_sparse_categorical_accuracy: 0.9585 - val_sparse_categorical_crossentropy: 0.1697 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 18s 185ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9927 - sparse_categorical_crossentropy: 0.0268 - val_loss: 0.1468 - val_sparse_categorical_accuracy: 0.9655 - val_sparse_categorical_crossentropy: 0.1468 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9938 - sparse_categorical_crossentropy: 0.0231 - val_loss: 0.1586 - val_sparse_categorical_accuracy: 0.9653 - val_sparse_categorical_crossentropy: 0.1586 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 17s 183ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.9905 - sparse_categorical_crossentropy: 0.0317 - val_loss: 0.1902 - val_sparse_categorical_accuracy: 0.9585 - val_sparse_categorical_crossentropy: 0.1902 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0329 - sparse_categorical_accuracy: 0.9901 - sparse_categorical_crossentropy: 0.0329\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 18s 188ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9900 - sparse_categorical_crossentropy: 0.0329 - val_loss: 0.1795 - val_sparse_categorical_accuracy: 0.9585 - val_sparse_categorical_crossentropy: 0.1795 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9901 - sparse_categorical_crossentropy: 0.0338 - val_loss: 0.1542 - val_sparse_categorical_accuracy: 0.9642 - val_sparse_categorical_crossentropy: 0.1542 - lr: 0.0025\n",
      "Epoch 18/200\n",
      "90/90 [==============================] - 18s 190ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9950 - sparse_categorical_crossentropy: 0.0187 - val_loss: 0.1547 - val_sparse_categorical_accuracy: 0.9664 - val_sparse_categorical_crossentropy: 0.1547 - lr: 0.0025\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - 17s 179ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9957 - sparse_categorical_crossentropy: 0.0159 - val_loss: 0.1595 - val_sparse_categorical_accuracy: 0.9670 - val_sparse_categorical_crossentropy: 0.1595 - lr: 0.0025\n",
      "Epoch 20/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9962 - sparse_categorical_crossentropy: 0.0144 - val_loss: 0.1697 - val_sparse_categorical_accuracy: 0.9656 - val_sparse_categorical_crossentropy: 0.1697 - lr: 0.0025\n",
      "Epoch 21/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9963 - sparse_categorical_crossentropy: 0.0138 - val_loss: 0.1720 - val_sparse_categorical_accuracy: 0.9667 - val_sparse_categorical_crossentropy: 0.1720 - lr: 0.0025\n",
      "Epoch 22/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0130 - sparse_categorical_accuracy: 0.9965 - sparse_categorical_crossentropy: 0.0130\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0130 - sparse_categorical_accuracy: 0.9965 - sparse_categorical_crossentropy: 0.0130 - val_loss: 0.1755 - val_sparse_categorical_accuracy: 0.9668 - val_sparse_categorical_crossentropy: 0.1755 - lr: 0.0025\n",
      "Epoch 23/200\n",
      "90/90 [==============================] - 17s 184ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9966 - sparse_categorical_crossentropy: 0.0121 - val_loss: 0.1824 - val_sparse_categorical_accuracy: 0.9668 - val_sparse_categorical_crossentropy: 0.1824 - lr: 0.0012\n",
      "Epoch 24/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9966 - sparse_categorical_crossentropy: 0.0121 - val_loss: 0.1852 - val_sparse_categorical_accuracy: 0.9670 - val_sparse_categorical_crossentropy: 0.1852 - lr: 0.0012\n",
      "26/26 [==============================] - 3s 95ms/step - loss: 0.1611 - sparse_categorical_accuracy: 0.9668 - sparse_categorical_crossentropy: 0.1611\n",
      "26/26 [==============================] - 4s 85ms/step\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 24s 195ms/step - loss: 1.4773 - sparse_categorical_accuracy: 0.6285 - sparse_categorical_crossentropy: 1.4773 - val_loss: 1.4103 - val_sparse_categorical_accuracy: 0.6410 - val_sparse_categorical_crossentropy: 1.4103 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.7350 - sparse_categorical_accuracy: 0.7762 - sparse_categorical_crossentropy: 0.7350 - val_loss: 1.5883 - val_sparse_categorical_accuracy: 0.6324 - val_sparse_categorical_crossentropy: 1.5883 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 18s 186ms/step - loss: 0.3082 - sparse_categorical_accuracy: 0.9051 - sparse_categorical_crossentropy: 0.3082 - val_loss: 1.5302 - val_sparse_categorical_accuracy: 0.5796 - val_sparse_categorical_crossentropy: 1.5302 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 18s 188ms/step - loss: 0.2332 - sparse_categorical_accuracy: 0.9286 - sparse_categorical_crossentropy: 0.2332 - val_loss: 0.8467 - val_sparse_categorical_accuracy: 0.7608 - val_sparse_categorical_crossentropy: 0.8467 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 18s 185ms/step - loss: 0.1526 - sparse_categorical_accuracy: 0.9537 - sparse_categorical_crossentropy: 0.1526 - val_loss: 0.2829 - val_sparse_categorical_accuracy: 0.9177 - val_sparse_categorical_crossentropy: 0.2829 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 18s 191ms/step - loss: 0.1127 - sparse_categorical_accuracy: 0.9663 - sparse_categorical_crossentropy: 0.1127 - val_loss: 0.1690 - val_sparse_categorical_accuracy: 0.9506 - val_sparse_categorical_crossentropy: 0.1690 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.1093 - sparse_categorical_accuracy: 0.9667 - sparse_categorical_crossentropy: 0.1093 - val_loss: 0.1872 - val_sparse_categorical_accuracy: 0.9496 - val_sparse_categorical_crossentropy: 0.1872 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 18s 187ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9727 - sparse_categorical_crossentropy: 0.0921 - val_loss: 0.1941 - val_sparse_categorical_accuracy: 0.9440 - val_sparse_categorical_crossentropy: 0.1941 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0788 - sparse_categorical_accuracy: 0.9766 - sparse_categorical_crossentropy: 0.0788\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 18s 184ms/step - loss: 0.0787 - sparse_categorical_accuracy: 0.9766 - sparse_categorical_crossentropy: 0.0787 - val_loss: 0.1746 - val_sparse_categorical_accuracy: 0.9489 - val_sparse_categorical_crossentropy: 0.1746 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 18s 185ms/step - loss: 0.0602 - sparse_categorical_accuracy: 0.9827 - sparse_categorical_crossentropy: 0.0602 - val_loss: 0.1173 - val_sparse_categorical_accuracy: 0.9691 - val_sparse_categorical_crossentropy: 0.1173 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 18s 184ms/step - loss: 0.0366 - sparse_categorical_accuracy: 0.9898 - sparse_categorical_crossentropy: 0.0366 - val_loss: 0.1261 - val_sparse_categorical_accuracy: 0.9694 - val_sparse_categorical_crossentropy: 0.1261 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0301 - sparse_categorical_accuracy: 0.9920 - sparse_categorical_crossentropy: 0.0301 - val_loss: 0.1346 - val_sparse_categorical_accuracy: 0.9671 - val_sparse_categorical_crossentropy: 0.1346 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 18s 188ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.9932 - sparse_categorical_crossentropy: 0.0259 - val_loss: 0.1467 - val_sparse_categorical_accuracy: 0.9662 - val_sparse_categorical_crossentropy: 0.1467 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0237 - sparse_categorical_accuracy: 0.9934 - sparse_categorical_crossentropy: 0.0237\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9934 - sparse_categorical_crossentropy: 0.0237 - val_loss: 0.1518 - val_sparse_categorical_accuracy: 0.9668 - val_sparse_categorical_crossentropy: 0.1518 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9944 - sparse_categorical_crossentropy: 0.0204 - val_loss: 0.1514 - val_sparse_categorical_accuracy: 0.9684 - val_sparse_categorical_crossentropy: 0.1514 - lr: 0.0025\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 18s 189ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9950 - sparse_categorical_crossentropy: 0.0179 - val_loss: 0.1600 - val_sparse_categorical_accuracy: 0.9685 - val_sparse_categorical_crossentropy: 0.1600 - lr: 0.0025\n",
      "26/26 [==============================] - 3s 97ms/step - loss: 0.1377 - sparse_categorical_accuracy: 0.9636 - sparse_categorical_crossentropy: 0.1377\n",
      "26/26 [==============================] - 4s 90ms/step\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 24s 197ms/step - loss: 0.9917 - sparse_categorical_accuracy: 0.7101 - sparse_categorical_crossentropy: 0.9917 - val_loss: 1.3479 - val_sparse_categorical_accuracy: 0.6472 - val_sparse_categorical_crossentropy: 1.3479 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.4429 - sparse_categorical_accuracy: 0.8613 - sparse_categorical_crossentropy: 0.4429 - val_loss: 1.1641 - val_sparse_categorical_accuracy: 0.6528 - val_sparse_categorical_crossentropy: 1.1641 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 19s 199ms/step - loss: 0.2986 - sparse_categorical_accuracy: 0.9060 - sparse_categorical_crossentropy: 0.2986 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.8962 - val_sparse_categorical_crossentropy: 0.3307 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 18s 193ms/step - loss: 0.2166 - sparse_categorical_accuracy: 0.9331 - sparse_categorical_crossentropy: 0.2166 - val_loss: 0.2332 - val_sparse_categorical_accuracy: 0.9260 - val_sparse_categorical_crossentropy: 0.2332 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 19s 199ms/step - loss: 0.1686 - sparse_categorical_accuracy: 0.9487 - sparse_categorical_crossentropy: 0.1686 - val_loss: 0.1986 - val_sparse_categorical_accuracy: 0.9389 - val_sparse_categorical_crossentropy: 0.1986 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.1510 - sparse_categorical_accuracy: 0.9523 - sparse_categorical_crossentropy: 0.1510 - val_loss: 0.1566 - val_sparse_categorical_accuracy: 0.9524 - val_sparse_categorical_crossentropy: 0.1566 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.1143 - sparse_categorical_accuracy: 0.9655 - sparse_categorical_crossentropy: 0.1143 - val_loss: 0.1635 - val_sparse_categorical_accuracy: 0.9546 - val_sparse_categorical_crossentropy: 0.1635 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 19s 200ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9643 - sparse_categorical_crossentropy: 0.1147 - val_loss: 0.1792 - val_sparse_categorical_accuracy: 0.9493 - val_sparse_categorical_crossentropy: 0.1792 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0883 - sparse_categorical_accuracy: 0.9730 - sparse_categorical_crossentropy: 0.0883 - val_loss: 0.1691 - val_sparse_categorical_accuracy: 0.9486 - val_sparse_categorical_crossentropy: 0.1691 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0933 - sparse_categorical_accuracy: 0.9711 - sparse_categorical_crossentropy: 0.0933 - val_loss: 0.1504 - val_sparse_categorical_accuracy: 0.9569 - val_sparse_categorical_crossentropy: 0.1504 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 19s 195ms/step - loss: 0.1000 - sparse_categorical_accuracy: 0.9691 - sparse_categorical_crossentropy: 0.1000 - val_loss: 0.1627 - val_sparse_categorical_accuracy: 0.9529 - val_sparse_categorical_crossentropy: 0.1627 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 19s 195ms/step - loss: 0.0848 - sparse_categorical_accuracy: 0.9741 - sparse_categorical_crossentropy: 0.0848 - val_loss: 0.1409 - val_sparse_categorical_accuracy: 0.9584 - val_sparse_categorical_crossentropy: 0.1409 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0863 - sparse_categorical_accuracy: 0.9736 - sparse_categorical_crossentropy: 0.0863 - val_loss: 0.1459 - val_sparse_categorical_accuracy: 0.9601 - val_sparse_categorical_crossentropy: 0.1459 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 18s 185ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9809 - sparse_categorical_crossentropy: 0.0633 - val_loss: 0.1704 - val_sparse_categorical_accuracy: 0.9581 - val_sparse_categorical_crossentropy: 0.1704 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0546 - sparse_categorical_accuracy: 0.9832 - sparse_categorical_crossentropy: 0.0546 - val_loss: 0.1675 - val_sparse_categorical_accuracy: 0.9570 - val_sparse_categorical_crossentropy: 0.1675 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9795 - sparse_categorical_crossentropy: 0.0657 - val_loss: 0.1547 - val_sparse_categorical_accuracy: 0.9610 - val_sparse_categorical_crossentropy: 0.1547 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0617 - sparse_categorical_accuracy: 0.9808 - sparse_categorical_crossentropy: 0.0617 - val_loss: 0.1724 - val_sparse_categorical_accuracy: 0.9582 - val_sparse_categorical_crossentropy: 0.1724 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.1246 - sparse_categorical_accuracy: 0.9616 - sparse_categorical_crossentropy: 0.1246 - val_loss: 0.2629 - val_sparse_categorical_accuracy: 0.9277 - val_sparse_categorical_crossentropy: 0.2629 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0954 - sparse_categorical_accuracy: 0.9701 - sparse_categorical_crossentropy: 0.0954\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 19s 195ms/step - loss: 0.0954 - sparse_categorical_accuracy: 0.9701 - sparse_categorical_crossentropy: 0.0954 - val_loss: 0.1584 - val_sparse_categorical_accuracy: 0.9562 - val_sparse_categorical_crossentropy: 0.1584 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0472 - sparse_categorical_accuracy: 0.9860 - sparse_categorical_crossentropy: 0.0472 - val_loss: 0.1229 - val_sparse_categorical_accuracy: 0.9664 - val_sparse_categorical_crossentropy: 0.1229 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9913 - sparse_categorical_crossentropy: 0.0309 - val_loss: 0.1376 - val_sparse_categorical_accuracy: 0.9668 - val_sparse_categorical_crossentropy: 0.1376 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "90/90 [==============================] - 19s 199ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9938 - sparse_categorical_crossentropy: 0.0226 - val_loss: 0.1539 - val_sparse_categorical_accuracy: 0.9667 - val_sparse_categorical_crossentropy: 0.1539 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.9927 - sparse_categorical_crossentropy: 0.0254 - val_loss: 0.1545 - val_sparse_categorical_accuracy: 0.9662 - val_sparse_categorical_crossentropy: 0.1545 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0280 - sparse_categorical_accuracy: 0.9916 - sparse_categorical_crossentropy: 0.0280 - val_loss: 0.1539 - val_sparse_categorical_accuracy: 0.9673 - val_sparse_categorical_crossentropy: 0.1539 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy: 0.0245 - val_loss: 0.1512 - val_sparse_categorical_accuracy: 0.9661 - val_sparse_categorical_crossentropy: 0.1512 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9942 - sparse_categorical_crossentropy: 0.0196 - val_loss: 0.1557 - val_sparse_categorical_accuracy: 0.9690 - val_sparse_categorical_crossentropy: 0.1557 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "90/90 [==============================] - 18s 191ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9943 - sparse_categorical_crossentropy: 0.0188 - val_loss: 0.1775 - val_sparse_categorical_accuracy: 0.9648 - val_sparse_categorical_crossentropy: 0.1775 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "90/90 [==============================] - 17s 183ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy: 0.0234 - val_loss: 0.2125 - val_sparse_categorical_accuracy: 0.9579 - val_sparse_categorical_crossentropy: 0.2125 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0599 - sparse_categorical_accuracy: 0.9822 - sparse_categorical_crossentropy: 0.0599\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 19s 199ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9823 - sparse_categorical_crossentropy: 0.0599 - val_loss: 0.1556 - val_sparse_categorical_accuracy: 0.9639 - val_sparse_categorical_crossentropy: 0.1556 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "90/90 [==============================] - 18s 194ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9935 - sparse_categorical_crossentropy: 0.0218 - val_loss: 0.1606 - val_sparse_categorical_accuracy: 0.9645 - val_sparse_categorical_crossentropy: 0.1606 - lr: 0.0025\n",
      "Epoch 31/200\n",
      "90/90 [==============================] - 19s 195ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9963 - sparse_categorical_crossentropy: 0.0143 - val_loss: 0.1731 - val_sparse_categorical_accuracy: 0.9647 - val_sparse_categorical_crossentropy: 0.1731 - lr: 0.0025\n",
      "26/26 [==============================] - 4s 103ms/step - loss: 0.1964 - sparse_categorical_accuracy: 0.9633 - sparse_categorical_crossentropy: 0.1964\n",
      "26/26 [==============================] - 5s 96ms/step\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 25s 215ms/step - loss: 0.9173 - sparse_categorical_accuracy: 0.7297 - sparse_categorical_crossentropy: 0.9173 - val_loss: 1.0723 - val_sparse_categorical_accuracy: 0.6553 - val_sparse_categorical_crossentropy: 1.0723 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.3657 - sparse_categorical_accuracy: 0.8861 - sparse_categorical_crossentropy: 0.3657 - val_loss: 0.4028 - val_sparse_categorical_accuracy: 0.8741 - val_sparse_categorical_crossentropy: 0.4028 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.2302 - sparse_categorical_accuracy: 0.9285 - sparse_categorical_crossentropy: 0.2302 - val_loss: 0.5108 - val_sparse_categorical_accuracy: 0.8448 - val_sparse_categorical_crossentropy: 0.5108 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.1622 - sparse_categorical_accuracy: 0.9504 - sparse_categorical_crossentropy: 0.1622 - val_loss: 0.2986 - val_sparse_categorical_accuracy: 0.9047 - val_sparse_categorical_crossentropy: 0.2986 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.1426 - sparse_categorical_accuracy: 0.9562 - sparse_categorical_crossentropy: 0.1426 - val_loss: 0.2004 - val_sparse_categorical_accuracy: 0.9374 - val_sparse_categorical_crossentropy: 0.2004 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.1566 - sparse_categorical_accuracy: 0.9518 - sparse_categorical_crossentropy: 0.1566 - val_loss: 0.1758 - val_sparse_categorical_accuracy: 0.9480 - val_sparse_categorical_crossentropy: 0.1758 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.1073 - sparse_categorical_accuracy: 0.9679 - sparse_categorical_crossentropy: 0.1073 - val_loss: 0.1784 - val_sparse_categorical_accuracy: 0.9455 - val_sparse_categorical_crossentropy: 0.1784 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0961 - sparse_categorical_accuracy: 0.9708 - sparse_categorical_crossentropy: 0.0961 - val_loss: 0.1803 - val_sparse_categorical_accuracy: 0.9432 - val_sparse_categorical_crossentropy: 0.1803 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0912 - sparse_categorical_accuracy: 0.9722 - sparse_categorical_crossentropy: 0.0912 - val_loss: 0.1540 - val_sparse_categorical_accuracy: 0.9602 - val_sparse_categorical_crossentropy: 0.1540 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0857 - sparse_categorical_accuracy: 0.9740 - sparse_categorical_crossentropy: 0.0857 - val_loss: 0.1567 - val_sparse_categorical_accuracy: 0.9536 - val_sparse_categorical_crossentropy: 0.1567 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - 19s 196ms/step - loss: 0.0822 - sparse_categorical_accuracy: 0.9748 - sparse_categorical_crossentropy: 0.0822 - val_loss: 0.1742 - val_sparse_categorical_accuracy: 0.9530 - val_sparse_categorical_crossentropy: 0.1742 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0634 - sparse_categorical_accuracy: 0.9812 - sparse_categorical_crossentropy: 0.0634\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9812 - sparse_categorical_crossentropy: 0.0633 - val_loss: 0.1526 - val_sparse_categorical_accuracy: 0.9589 - val_sparse_categorical_crossentropy: 0.1526 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0433 - sparse_categorical_accuracy: 0.9875 - sparse_categorical_crossentropy: 0.0433 - val_loss: 0.1272 - val_sparse_categorical_accuracy: 0.9674 - val_sparse_categorical_crossentropy: 0.1272 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 19s 199ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9921 - sparse_categorical_crossentropy: 0.0293 - val_loss: 0.1350 - val_sparse_categorical_accuracy: 0.9656 - val_sparse_categorical_crossentropy: 0.1350 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9936 - sparse_categorical_crossentropy: 0.0245 - val_loss: 0.1446 - val_sparse_categorical_accuracy: 0.9688 - val_sparse_categorical_crossentropy: 0.1446 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9941 - sparse_categorical_crossentropy: 0.0214 - val_loss: 0.1546 - val_sparse_categorical_accuracy: 0.9679 - val_sparse_categorical_crossentropy: 0.1546 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 19s 197ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9944 - sparse_categorical_crossentropy: 0.0201 - val_loss: 0.1651 - val_sparse_categorical_accuracy: 0.9668 - val_sparse_categorical_crossentropy: 0.1651 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0203 - sparse_categorical_accuracy: 0.9941 - sparse_categorical_crossentropy: 0.0203\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9941 - sparse_categorical_crossentropy: 0.0204 - val_loss: 0.2696 - val_sparse_categorical_accuracy: 0.9529 - val_sparse_categorical_crossentropy: 0.2696 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - 19s 198ms/step - loss: 0.0480 - sparse_categorical_accuracy: 0.9862 - sparse_categorical_crossentropy: 0.0480 - val_loss: 0.1496 - val_sparse_categorical_accuracy: 0.9664 - val_sparse_categorical_crossentropy: 0.1496 - lr: 0.0025\n",
      "Epoch 20/200\n",
      "90/90 [==============================] - 19s 199ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9930 - sparse_categorical_crossentropy: 0.0237 - val_loss: 0.1629 - val_sparse_categorical_accuracy: 0.9665 - val_sparse_categorical_crossentropy: 0.1629 - lr: 0.0025\n",
      "26/26 [==============================] - 4s 107ms/step - loss: 0.1629 - sparse_categorical_accuracy: 0.9654 - sparse_categorical_crossentropy: 0.1629\n",
      "26/26 [==============================] - 5s 104ms/step\n",
      "Epoch 1/200\n",
      "90/90 [==============================] - 26s 214ms/step - loss: 1.4312 - sparse_categorical_accuracy: 0.6259 - sparse_categorical_crossentropy: 1.4312 - val_loss: 1.2619 - val_sparse_categorical_accuracy: 0.6360 - val_sparse_categorical_crossentropy: 1.2619 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - 18s 190ms/step - loss: 0.7608 - sparse_categorical_accuracy: 0.7559 - sparse_categorical_crossentropy: 0.7608 - val_loss: 1.5829 - val_sparse_categorical_accuracy: 0.6312 - val_sparse_categorical_crossentropy: 1.5829 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "90/90 [==============================] - 18s 186ms/step - loss: 0.3656 - sparse_categorical_accuracy: 0.8860 - sparse_categorical_crossentropy: 0.3656 - val_loss: 1.3107 - val_sparse_categorical_accuracy: 0.6513 - val_sparse_categorical_crossentropy: 1.3107 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.2780 - sparse_categorical_accuracy: 0.9149 - sparse_categorical_crossentropy: 0.2780 - val_loss: 1.5368 - val_sparse_categorical_accuracy: 0.6304 - val_sparse_categorical_crossentropy: 1.5368 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.1809 - sparse_categorical_accuracy: 0.9452 - sparse_categorical_crossentropy: 0.1809 - val_loss: 0.6739 - val_sparse_categorical_accuracy: 0.7943 - val_sparse_categorical_crossentropy: 0.6739 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.1423 - sparse_categorical_accuracy: 0.9565 - sparse_categorical_crossentropy: 0.1423 - val_loss: 0.1910 - val_sparse_categorical_accuracy: 0.9406 - val_sparse_categorical_crossentropy: 0.1910 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "90/90 [==============================] - 18s 183ms/step - loss: 0.1070 - sparse_categorical_accuracy: 0.9674 - sparse_categorical_crossentropy: 0.1070 - val_loss: 0.1635 - val_sparse_categorical_accuracy: 0.9489 - val_sparse_categorical_crossentropy: 0.1635 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0886 - sparse_categorical_accuracy: 0.9733 - sparse_categorical_crossentropy: 0.0886 - val_loss: 0.1203 - val_sparse_categorical_accuracy: 0.9608 - val_sparse_categorical_crossentropy: 0.1203 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0741 - sparse_categorical_accuracy: 0.9780 - sparse_categorical_crossentropy: 0.0741 - val_loss: 0.1466 - val_sparse_categorical_accuracy: 0.9547 - val_sparse_categorical_crossentropy: 0.1466 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0731 - sparse_categorical_accuracy: 0.9783 - sparse_categorical_crossentropy: 0.0731 - val_loss: 0.1482 - val_sparse_categorical_accuracy: 0.9587 - val_sparse_categorical_crossentropy: 0.1482 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0653 - sparse_categorical_accuracy: 0.9807 - sparse_categorical_crossentropy: 0.0653\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0653 - sparse_categorical_accuracy: 0.9807 - sparse_categorical_crossentropy: 0.0653 - val_loss: 0.1977 - val_sparse_categorical_accuracy: 0.9443 - val_sparse_categorical_crossentropy: 0.1977 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9879 - sparse_categorical_crossentropy: 0.0420 - val_loss: 0.1478 - val_sparse_categorical_accuracy: 0.9599 - val_sparse_categorical_crossentropy: 0.1478 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0305 - sparse_categorical_accuracy: 0.9920 - sparse_categorical_crossentropy: 0.0305 - val_loss: 0.1267 - val_sparse_categorical_accuracy: 0.9676 - val_sparse_categorical_crossentropy: 0.1267 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9932 - sparse_categorical_crossentropy: 0.0252 - val_loss: 0.1429 - val_sparse_categorical_accuracy: 0.9651 - val_sparse_categorical_crossentropy: 0.1429 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9902 - sparse_categorical_crossentropy: 0.0342 - val_loss: 0.1401 - val_sparse_categorical_accuracy: 0.9671 - val_sparse_categorical_crossentropy: 0.1401 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0937 - sparse_categorical_accuracy: 0.9726 - sparse_categorical_crossentropy: 0.0937\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0936 - sparse_categorical_accuracy: 0.9726 - sparse_categorical_crossentropy: 0.0936 - val_loss: 0.1800 - val_sparse_categorical_accuracy: 0.9513 - val_sparse_categorical_crossentropy: 0.1800 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0315 - sparse_categorical_accuracy: 0.9911 - sparse_categorical_crossentropy: 0.0315 - val_loss: 0.1195 - val_sparse_categorical_accuracy: 0.9688 - val_sparse_categorical_crossentropy: 0.1195 - lr: 0.0025\n",
      "Epoch 18/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9943 - sparse_categorical_crossentropy: 0.0218 - val_loss: 0.1244 - val_sparse_categorical_accuracy: 0.9702 - val_sparse_categorical_crossentropy: 0.1244 - lr: 0.0025\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - 18s 182ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9949 - sparse_categorical_crossentropy: 0.0191 - val_loss: 0.1286 - val_sparse_categorical_accuracy: 0.9707 - val_sparse_categorical_crossentropy: 0.1286 - lr: 0.0025\n",
      "Epoch 20/200\n",
      "90/90 [==============================] - 18s 182ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9952 - sparse_categorical_crossentropy: 0.0171 - val_loss: 0.1344 - val_sparse_categorical_accuracy: 0.9708 - val_sparse_categorical_crossentropy: 0.1344 - lr: 0.0025\n",
      "Epoch 21/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9955 - sparse_categorical_crossentropy: 0.0156 - val_loss: 0.1451 - val_sparse_categorical_accuracy: 0.9714 - val_sparse_categorical_crossentropy: 0.1451 - lr: 0.0025\n",
      "Epoch 22/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy: 0.0252 - val_loss: 0.1421 - val_sparse_categorical_accuracy: 0.9698 - val_sparse_categorical_crossentropy: 0.1421 - lr: 0.0025\n",
      "Epoch 23/200\n",
      "90/90 [==============================] - 17s 180ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9949 - sparse_categorical_crossentropy: 0.0180 - val_loss: 0.1501 - val_sparse_categorical_accuracy: 0.9679 - val_sparse_categorical_crossentropy: 0.1501 - lr: 0.0025\n",
      "Epoch 24/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0164 - sparse_categorical_accuracy: 0.9954 - sparse_categorical_crossentropy: 0.0164\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9954 - sparse_categorical_crossentropy: 0.0164 - val_loss: 0.1627 - val_sparse_categorical_accuracy: 0.9688 - val_sparse_categorical_crossentropy: 0.1627 - lr: 0.0025\n",
      "Epoch 25/200\n",
      "90/90 [==============================] - 17s 182ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9960 - sparse_categorical_crossentropy: 0.0147 - val_loss: 0.1597 - val_sparse_categorical_accuracy: 0.9701 - val_sparse_categorical_crossentropy: 0.1597 - lr: 0.0012\n",
      "Epoch 26/200\n",
      "90/90 [==============================] - 17s 181ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9965 - sparse_categorical_crossentropy: 0.0126 - val_loss: 0.1626 - val_sparse_categorical_accuracy: 0.9701 - val_sparse_categorical_crossentropy: 0.1626 - lr: 0.0012\n",
      "26/26 [==============================] - 4s 108ms/step - loss: 0.1784 - sparse_categorical_accuracy: 0.9655 - sparse_categorical_crossentropy: 0.1784\n",
      "26/26 [==============================] - 5s 102ms/step\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SEEDS = list(range(5))\n",
    "results = []\n",
    "for seed in TRAINING_SEEDS:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    model = SpeechModels.AttRNNSpeechModel(N_CLASS, samplingrate = 16000, inputLength = None)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "        loss=losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[metrics.SparseCategoricalAccuracy(), metrics.SparseCategoricalCrossentropy()]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(EXPERIMENT_NAME, f\"history_{seed}.pkl\"), \"wb\") as file:\n",
    "        pickle.dump(history.history, file)\n",
    "\n",
    "    eval_results = model.evaluate(test_ds)\n",
    "\n",
    "    predictions = model.predict(test_ds)\n",
    "    with open(os.path.join(EXPERIMENT_NAME, f\"predictions_{seed}.pkl\"), \"wb\") as file:\n",
    "        pickle.dump(predictions, file)\n",
    "\n",
    "    results += [{\n",
    "        'seed': seed,\n",
    "        'results': dict(zip(model.metrics_names, eval_results))\n",
    "    }]\n",
    "    gc.collect()\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results = pd.concat([results.drop([\"results\"], axis=1), results[\"results\"].apply(pd.Series)], axis=1)\n",
    "results.to_csv(os.path.join(EXPERIMENT_NAME, 'results.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T22:21:19.079706Z",
     "end_time": "2023-04-22T22:27:33.390217Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T22:27:33.393217Z",
     "end_time": "2023-04-22T22:27:33.405218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   seed      loss  sparse_categorical_accuracy  \\\n0     0  0.161144                     0.966830   \n1     1  0.137709                     0.963606   \n2     2  0.196414                     0.963298   \n3     3  0.162859                     0.965372   \n4     4  0.178363                     0.965525   \n\n   sparse_categorical_crossentropy  \n0                         0.161144  \n1                         0.137709  \n2                         0.196414  \n3                         0.162859  \n4                         0.178363  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seed</th>\n      <th>loss</th>\n      <th>sparse_categorical_accuracy</th>\n      <th>sparse_categorical_crossentropy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.161144</td>\n      <td>0.966830</td>\n      <td>0.161144</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.137709</td>\n      <td>0.963606</td>\n      <td>0.137709</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.196414</td>\n      <td>0.963298</td>\n      <td>0.196414</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.162859</td>\n      <td>0.965372</td>\n      <td>0.162859</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.178363</td>\n      <td>0.965525</td>\n      <td>0.178363</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
