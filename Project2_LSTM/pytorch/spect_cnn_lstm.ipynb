{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from load_data import AudioTrainDataset, PaddingZeros, CustomSpectogram, TargetEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:49.194135Z",
     "end_time": "2023-04-22T18:12:53.299437Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 'bed',\n 2: 'bird',\n 3: 'cat',\n 4: 'dog',\n 5: 'down',\n 6: 'eight',\n 7: 'five',\n 8: 'four',\n 9: 'go',\n 10: 'happy',\n 11: 'house',\n 12: 'left',\n 13: 'marvin',\n 14: 'nine',\n 15: 'no',\n 16: 'off',\n 17: 'on',\n 18: 'one',\n 19: 'right',\n 20: 'seven',\n 21: 'sheila',\n 22: 'silence',\n 23: 'six',\n 24: 'stop',\n 25: 'three',\n 26: 'tree',\n 27: 'two',\n 28: 'up',\n 29: 'wow',\n 30: 'yes',\n 31: 'zero'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\"tensorflow-speech-recognition-challenge\", \"train\", \"audio\")\n",
    "dataset = AudioTrainDataset(DATA_PATH)\n",
    "\n",
    "labels_list, labels_dict = dataset.find_classes(DATA_PATH)\n",
    "labels_dict = {idx: name for name, idx in labels_dict.items()}\n",
    "labels_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:53.302437Z",
     "end_time": "2023-04-22T18:12:53.610437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "NUM_WORKERS = 6\n",
    "BATCH_SIZE = 512"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:53.610437Z",
     "end_time": "2023-04-22T18:12:53.625437Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    PaddingZeros(16000),\n",
    "    CustomSpectogram(n_fft=1024, power=2),\n",
    "])\n",
    "\n",
    "features_dataset = AudioTrainDataset(DATA_PATH, transform=transforms,\n",
    "                                     target_transform=TargetEncoder(class_dict=labels_dict))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:53.627437Z",
     "end_time": "2023-04-22T18:12:53.935436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(45587, 6512, 13024)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(features_dataset, [0.7, 0.1, 0.2],\n",
    "                                                                           generator=gen)\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:53.937437Z",
     "end_time": "2023-04-22T18:12:53.950437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(90, 13, 26)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True,\n",
    "                                  generator=torch.random.manual_seed(123))\n",
    "valid_dataset_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "test_dataset_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "len(train_dataset_loader), len(valid_dataset_loader), len(test_dataset_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:53.952436Z",
     "end_time": "2023-04-22T18:12:53.995437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MyLSTM(lightning.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_features_size, # number of frequencies of spectogram\n",
    "                 input_sequence_size, # length of spectogram\n",
    "                 hidden_size,\n",
    "                 conv_channels_out,\n",
    "                 conv_kernel_size,\n",
    "                 target_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv = torch.nn.Conv1d(input_sequence_size, conv_channels_out, kernel_size=conv_kernel_size, groups=input_sequence_size)\n",
    "        lstm_input_size = input_features_size - (conv_kernel_size - 1)\n",
    "        self.lstm = torch.nn.LSTM(lstm_input_size, hidden_size, num_layers=1, batch_first=True, dropout=0.3)\n",
    "        self.hidden2label = torch.nn.Linear(hidden_size, target_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=target_size)\n",
    "        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=target_size)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=target_size)\n",
    "        self.test_conf_mat = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_squeeze = x.squeeze()\n",
    "        x = self.conv(x_squeeze)\n",
    "        lstm_out, (hc, _) = self.lstm(x)\n",
    "        label_space = self.hidden2label(hc.squeeze())\n",
    "        return self.softmax(label_space)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        self.train_acc(y_hat, torch.argmax(y, dim=-1))\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc_step\", self.train_acc)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_acc', self.train_acc)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx = 0):\n",
    "        x, y = batch\n",
    "        return self(x)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        self.valid_acc(y_hat, torch.argmax(y, dim=-1))\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', self.valid_acc, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        y_class = torch.argmax(y, dim=-1)\n",
    "        self.test_acc(y_hat, y_class)\n",
    "        self.test_conf_mat(y_hat, y_class)\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.3, patience=3),\n",
    "                \"monitor\": \"val_loss\",\n",
    "            }\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:53.966436Z",
     "end_time": "2023-04-22T18:12:53.998437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 32, 513])\n"
     ]
    }
   ],
   "source": [
    "for batch_x,  batch_y in train_dataset_loader:\n",
    "    print(batch_x.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:53.982437Z",
     "end_time": "2023-04-22T18:13:08.731605Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piotrek\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = MyLSTM(513, 32, 128, 32, 8, 12)\n",
    "for batch_x,  batch_y in train_dataset_loader:\n",
    "    y_hat = model(batch_x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:13:08.732606Z",
     "end_time": "2023-04-22T18:13:23.798641Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                      | Params\n",
      "------------------------------------------------------------\n",
      "0 | conv          | Conv1d                    | 1.2 K \n",
      "1 | lstm          | LSTM                      | 325 K \n",
      "2 | hidden2label  | Linear                    | 1.5 K \n",
      "3 | softmax       | Softmax                   | 0     \n",
      "4 | train_acc     | MulticlassAccuracy        | 0     \n",
      "5 | valid_acc     | MulticlassAccuracy        | 0     \n",
      "6 | test_acc      | MulticlassAccuracy        | 0     \n",
      "7 | test_conf_mat | MulticlassConfusionMatrix | 0     \n",
      "------------------------------------------------------------\n",
      "328 K     Trainable params\n",
      "0         Non-trainable params\n",
      "328 K     Total params\n",
      "1.313     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 90/90 [00:19<00:00,  4.66it/s, v_num=29, train_loss_step=1.990]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/13 [00:00<00:02,  5.88it/s]\u001B[A\n",
      "Validation DataLoader 0:  15%|█▌        | 2/13 [00:00<00:01,  9.57it/s]\u001B[A\n",
      "Validation DataLoader 0:  23%|██▎       | 3/13 [00:00<00:00, 12.50it/s]\u001B[A\n",
      "Validation DataLoader 0:  31%|███       | 4/13 [00:00<00:00, 14.87it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 16.78it/s]\u001B[A\n",
      "Validation DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 18.24it/s]\u001B[A\n",
      "Validation DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 19.50it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 20.46it/s]\u001B[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 21.28it/s]\u001B[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 21.98it/s]\u001B[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 22.73it/s]\u001B[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 23.39it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 90/90 [00:34<00:00,  2.64it/s, v_num=29, train_loss_step=1.990]\n",
      "Epoch 1: 100%|██████████| 90/90 [00:19<00:00,  4.52it/s, v_num=29, train_loss_step=1.990, train_loss_epoch=2.000]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/13 [00:00<00:02,  4.59it/s]\u001B[A\n",
      "Validation DataLoader 0:  15%|█▌        | 2/13 [00:00<00:01,  8.03it/s]\u001B[A\n",
      "Validation DataLoader 0:  23%|██▎       | 3/13 [00:00<00:00, 10.68it/s]\u001B[A\n",
      "Validation DataLoader 0:  31%|███       | 4/13 [00:00<00:00, 12.78it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 14.58it/s]\u001B[A\n",
      "Validation DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 16.13it/s]\u001B[A\n",
      "Validation DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 17.42it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 18.44it/s]\u001B[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 19.40it/s]\u001B[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 20.21it/s]\u001B[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 20.96it/s]\u001B[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 21.55it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 90/90 [00:34<00:00,  2.59it/s, v_num=29, train_loss_step=1.990, train_loss_epoch=2.000]\n",
      "Epoch 1: 100%|██████████| 90/90 [00:34<00:00,  2.59it/s, v_num=29, train_loss_step=1.990, train_loss_epoch=1.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 90/90 [00:34<00:00,  2.59it/s, v_num=29, train_loss_step=1.990, train_loss_epoch=1.990]\n"
     ]
    }
   ],
   "source": [
    "model = MyLSTM(513, 32,  128, 128, 8, 12)\n",
    "trainer = lightning.Trainer(max_epochs=2, logger=True)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "trainer.fit(model, train_dataloaders=train_dataset_loader, val_dataloaders=valid_dataset_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:13:23.800642Z",
     "end_time": "2023-04-22T18:15:20.220138Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "C:\\Users\\Piotrek\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Piotrek\\anaconda3\\envs\\pytorch\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory cnn_lstm\\lightning_logs\\version_0\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                      | Params\n",
      "------------------------------------------------------------\n",
      "0 | conv          | Conv1d                    | 288   \n",
      "1 | lstm          | LSTM                      | 325 K \n",
      "2 | hidden2label  | Linear                    | 1.5 K \n",
      "3 | softmax       | Softmax                   | 0     \n",
      "4 | train_acc     | MulticlassAccuracy        | 0     \n",
      "5 | valid_acc     | MulticlassAccuracy        | 0     \n",
      "6 | test_acc      | MulticlassAccuracy        | 0     \n",
      "7 | test_conf_mat | MulticlassConfusionMatrix | 0     \n",
      "------------------------------------------------------------\n",
      "327 K     Trainable params\n",
      "0         Non-trainable params\n",
      "327 K     Total params\n",
      "1.310     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 90/90 [00:19<00:00,  4.59it/s, v_num=0, train_loss_step=2.040]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/13 [00:00<00:02,  5.92it/s]\u001B[A\n",
      "Validation DataLoader 0:  15%|█▌        | 2/13 [00:00<00:01, 10.05it/s]\u001B[A\n",
      "Validation DataLoader 0:  23%|██▎       | 3/13 [00:00<00:00, 13.27it/s]\u001B[A\n",
      "Validation DataLoader 0:  31%|███       | 4/13 [00:00<00:00, 15.87it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 18.05it/s]\u001B[A\n",
      "Validation DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 19.86it/s]\u001B[A\n",
      "Validation DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 18.37it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 19.70it/s]\u001B[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 20.74it/s]\u001B[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 21.64it/s]\u001B[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 22.45it/s]\u001B[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 23.14it/s]\u001B[A\n",
      "Epoch 0: 100%|██████████| 90/90 [00:34<00:00,  2.60it/s, v_num=0, train_loss_step=2.040]\n",
      "Epoch 0: 100%|██████████| 90/90 [00:34<00:00,  2.60it/s, v_num=0, train_loss_step=2.040, train_loss_epoch=2.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 90/90 [00:19<00:00,  4.51it/s, v_num=0, train_loss_step=2.040, train_loss_epoch=2.000]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/13 [00:00<00:02,  4.02it/s]\u001B[A\n",
      "Validation DataLoader 0:  15%|█▌        | 2/13 [00:00<00:01,  5.99it/s]\u001B[A\n",
      "Validation DataLoader 0:  23%|██▎       | 3/13 [00:00<00:01,  8.09it/s]\u001B[A\n",
      "Validation DataLoader 0:  31%|███       | 4/13 [00:00<00:00,  9.84it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 11.40it/s]\u001B[A\n",
      "Validation DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 12.83it/s]\u001B[A\n",
      "Validation DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 14.16it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 15.38it/s]\u001B[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 16.45it/s]\u001B[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 17.48it/s]\u001B[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 18.39it/s]\u001B[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 19.23it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 90/90 [00:35<00:00,  2.55it/s, v_num=0, train_loss_step=2.040, train_loss_epoch=2.000]\n",
      "Epoch 1: 100%|██████████| 90/90 [00:35<00:00,  2.55it/s, v_num=0, train_loss_step=2.040, train_loss_epoch=1.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 1.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 90/90 [00:19<00:00,  4.51it/s, v_num=0, train_loss_step=2.090, train_loss_epoch=1.990]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/13 [00:00<00:00, 45.46it/s]\u001B[A\n",
      "Validation DataLoader 0:  15%|█▌        | 2/13 [00:00<00:00, 27.03it/s]\u001B[A\n",
      "Validation DataLoader 0:  23%|██▎       | 3/13 [00:00<00:00, 27.52it/s]\u001B[A\n",
      "Validation DataLoader 0:  31%|███       | 4/13 [00:00<00:00, 26.82it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 26.15it/s]\u001B[A\n",
      "Validation DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 26.41it/s]\u001B[A\n",
      "Validation DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 20.82it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 21.55it/s]\u001B[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 21.83it/s]\u001B[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 22.31it/s]\u001B[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 22.66it/s]\u001B[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 22.75it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 90/90 [00:35<00:00,  2.57it/s, v_num=0, train_loss_step=2.090, train_loss_epoch=1.990]\n",
      "Epoch 3: 100%|██████████| 90/90 [00:20<00:00,  4.35it/s, v_num=0, train_loss_step=1.880, train_loss_epoch=1.990]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/13 [00:00<00:00, 19.61it/s]\u001B[A\n",
      "Validation DataLoader 0:  15%|█▌        | 2/13 [00:00<00:00, 20.83it/s]\u001B[A\n",
      "Validation DataLoader 0:  23%|██▎       | 3/13 [00:00<00:00, 23.08it/s]\u001B[A\n",
      "Validation DataLoader 0:  31%|███       | 4/13 [00:00<00:00, 23.81it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 25.00it/s]\u001B[A\n",
      "Validation DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 25.00it/s]\u001B[A\n",
      "Validation DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 19.72it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 20.56it/s]\u001B[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 21.32it/s]\u001B[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 21.74it/s]\u001B[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 22.22it/s]\u001B[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 22.38it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 90/90 [00:36<00:00,  2.47it/s, v_num=0, train_loss_step=1.880, train_loss_epoch=1.990]\n",
      "Epoch 3: 100%|██████████| 90/90 [00:36<00:00,  2.47it/s, v_num=0, train_loss_step=1.880, train_loss_epoch=1.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 1.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 90/90 [00:20<00:00,  4.29it/s, v_num=0, train_loss_step=1.990, train_loss_epoch=1.990]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   8%|▊         | 1/13 [00:00<00:02,  5.01it/s]\u001B[A\n",
      "Validation DataLoader 0:  15%|█▌        | 2/13 [00:00<00:01,  8.85it/s]\u001B[A\n",
      "Validation DataLoader 0:  23%|██▎       | 3/13 [00:00<00:00, 11.76it/s]\u001B[A\n",
      "Validation DataLoader 0:  31%|███       | 4/13 [00:00<00:00, 13.98it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 15.82it/s]\u001B[A\n",
      "Validation DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 17.44it/s]\u001B[A\n",
      "Validation DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 18.81it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 19.03it/s]\u001B[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 20.13it/s]\u001B[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 21.14it/s]\u001B[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 22.00it/s]\u001B[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 22.72it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 90/90 [00:36<00:00,  2.49it/s, v_num=0, train_loss_step=1.990, train_loss_epoch=1.990]\n",
      "Epoch 4: 100%|██████████| 90/90 [00:36<00:00,  2.49it/s, v_num=0, train_loss_step=1.990, train_loss_epoch=1.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 1.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/90 [00:00<?, ?it/s, v_num=0, train_loss_step=1.990, train_loss_epoch=1.990]         "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "predictions = []\n",
    "for i in range(5):\n",
    "    lightning.pytorch.seed_everything(i)\n",
    "    model = MyLSTM(513, 32,  128, 32, 8, 12)\n",
    "    early_stopping = lightning.pytorch.callbacks.EarlyStopping('val_loss', verbose=True, patience=5)\n",
    "    logger = lightning.pytorch.loggers.tensorboard.TensorBoardLogger(save_dir=\"cnn_lstm\", version=i)\n",
    "    trainer = lightning.Trainer(max_epochs=200, callbacks=[early_stopping], logger=logger)\n",
    "    trainer.fit(model, train_dataloaders=train_dataset_loader, val_dataloaders=valid_dataset_loader)\n",
    "    res = trainer.test(dataloaders=test_dataset_loader, ckpt_path='best')\n",
    "    test_pred_tensor = torch.cat(trainer.predict(dataloaders=test_dataset_loader, ckpt_path='best'))\n",
    "    results.append(res[0])\n",
    "    predictions.append(test_pred_tensor)\n",
    "torch.save(torch.stack(predictions), \"spectogram_cnn_lstm_predictions.ts\")\n",
    "pd.DataFrame(results).to_csv(\"spectogram_cnn_lstm_metrics.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:25:42.557530Z",
     "end_time": "2023-04-22T00:44:35.695274Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
