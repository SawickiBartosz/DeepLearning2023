{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "\n",
    "from Project2_LSTM.load_data import AudioTrainDataset, PaddingZeros, CustomSpectogram, TargetEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:24:34.461936Z",
     "end_time": "2023-04-19T22:24:38.222937Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 'bed',\n 2: 'bird',\n 3: 'cat',\n 4: 'dog',\n 5: 'down',\n 6: 'eight',\n 7: 'five',\n 8: 'four',\n 9: 'go',\n 10: 'happy',\n 11: 'house',\n 12: 'left',\n 13: 'marvin',\n 14: 'nine',\n 15: 'no',\n 16: 'off',\n 17: 'on',\n 18: 'one',\n 19: 'right',\n 20: 'seven',\n 21: 'sheila',\n 22: 'silence',\n 23: 'six',\n 24: 'stop',\n 25: 'three',\n 26: 'tree',\n 27: 'two',\n 28: 'up',\n 29: 'wow',\n 30: 'yes',\n 31: 'zero'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\"tensorflow-speech-recognition-challenge\", \"train\", \"audio\")\n",
    "dataset = AudioTrainDataset(DATA_PATH)\n",
    "\n",
    "labels_list, labels_dict = dataset.find_classes(DATA_PATH)\n",
    "labels_dict = {idx: name for name, idx in labels_dict.items()}\n",
    "labels_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:24:38.225937Z",
     "end_time": "2023-04-19T22:24:38.548925Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 128"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:24:38.550568Z",
     "end_time": "2023-04-19T22:24:38.592930Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    PaddingZeros(16000),\n",
    "    CustomSpectogram(n_fft=1024, power=2),\n",
    "])\n",
    "\n",
    "features_dataset = AudioTrainDataset(DATA_PATH, transform=transforms,\n",
    "                                     target_transform=TargetEncoder(class_dict=labels_dict))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:24:38.566930Z",
     "end_time": "2023-04-19T22:24:38.859404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(45587, 6512, 13024)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(features_dataset, [0.7, 0.1, 0.2],\n",
    "                                                                           generator=gen)\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:24:38.861404Z",
     "end_time": "2023-04-19T22:24:38.875404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(357, 51, 102)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True,\n",
    "                                  generator=torch.random.manual_seed(123))\n",
    "valid_dataset_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "test_dataset_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "len(train_dataset_loader), len(valid_dataset_loader), len(test_dataset_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:24:38.878404Z",
     "end_time": "2023-04-19T22:24:38.920540Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class MyLSTM(lightning.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_features_size, # number of frequencies of spectogram\n",
    "                 input_sequence_size, # length of spectogram\n",
    "                 hidden_size,\n",
    "                 conv_channels_out,\n",
    "                 conv_kernel_size,\n",
    "                 target_size):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv1d(input_sequence_size, conv_channels_out, kernel_size=conv_kernel_size, groups=input_sequence_size)\n",
    "        lstm_input_size = input_features_size - (conv_kernel_size - 1)\n",
    "        self.lstm = torch.nn.LSTM(lstm_input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.hidden2label = torch.nn.Linear(hidden_size, target_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=target_size)\n",
    "        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=target_size)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=target_size)\n",
    "        self.test_conf_mat = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_squeeze = x.squeeze()\n",
    "        x = self.conv(x_squeeze)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        label_space = self.hidden2label(lstm_out[:, -1])\n",
    "        return self.softmax(label_space)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        self.train_acc(y_hat, torch.argmax(y, dim=-1))\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc_step\", self.train_acc)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_acc', self.train_acc)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx = 0):\n",
    "        x, y = batch\n",
    "        return self(x)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        self.valid_acc(y_hat, torch.argmax(y, dim=-1))\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', self.valid_acc, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        y_class = torch.argmax(y, dim=-1)\n",
    "        self.test_acc(y_hat, y_class)\n",
    "        self.test_conf_mat(y_hat, y_class)\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.3, patience=3),\n",
    "                \"monitor\": \"val_loss\",\n",
    "            }\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:24:38.892405Z",
     "end_time": "2023-04-19T22:24:38.923541Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 32, 513])\n"
     ]
    }
   ],
   "source": [
    "for batch_x,  batch_y in train_dataset_loader:\n",
    "    print(batch_x.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model = MyLSTM(513, 32, 128, 32, 8, 12)\n",
    "for batch_x,  batch_y in train_dataset_loader:\n",
    "    y_hat = model(batch_x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type                      | Params\n",
      "------------------------------------------------------------\n",
      "0 | conv          | Conv1d                    | 1.2 K \n",
      "1 | lstm          | LSTM                      | 325 K \n",
      "2 | hidden2label  | Linear                    | 1.5 K \n",
      "3 | softmax       | Softmax                   | 0     \n",
      "4 | train_acc     | MulticlassAccuracy        | 0     \n",
      "5 | valid_acc     | MulticlassAccuracy        | 0     \n",
      "6 | test_acc      | MulticlassAccuracy        | 0     \n",
      "7 | test_conf_mat | MulticlassConfusionMatrix | 0     \n",
      "------------------------------------------------------------\n",
      "328 K     Trainable params\n",
      "0         Non-trainable params\n",
      "328 K     Total params\n",
      "1.313     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91bfa472ba2d4e849088be9bf2fc956f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54596af28a82420ca8f6ee811576ec8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MyLSTM(513, 32,  128, 128, 8, 12)\n",
    "trainer = lightning.Trainer(max_epochs=2, logger=True)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer.fit(model, train_dataloaders=train_dataset_loader, val_dataloaders=valid_dataset_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:39:40.123632Z",
     "end_time": "2023-04-19T22:42:41.676275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type                      | Params\n",
      "------------------------------------------------------------\n",
      "0 | conv          | Conv1d                    | 525 K \n",
      "1 | lstm          | LSTM                      | 79.4 K\n",
      "2 | hidden2label  | Linear                    | 1.5 K \n",
      "3 | softmax       | Softmax                   | 0     \n",
      "4 | train_acc     | MulticlassAccuracy        | 0     \n",
      "5 | valid_acc     | MulticlassAccuracy        | 0     \n",
      "6 | test_acc      | MulticlassAccuracy        | 0     \n",
      "7 | test_conf_mat | MulticlassConfusionMatrix | 0     \n",
      "------------------------------------------------------------\n",
      "606 K     Trainable params\n",
      "0         Non-trainable params\n",
      "606 K     Total params\n",
      "2.425     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "415257aad96f4831879bd2697224bc9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d3accb4b14e45c58d17b7e8d9d68d31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "998ff228fb224cb698c180b612a54dfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a4420e5a0364850a412a5f44226e475"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "Restoring states from the checkpoint path at E:\\Studies\\DataScience-1sem\\DeepLearning\\Project2_LSTM\\lightning_logs\\version_8\\checkpoints\\epoch=1-step=714.ckpt\n",
      "Loaded model weights from the checkpoint at E:\\Studies\\DataScience-1sem\\DeepLearning\\Project2_LSTM\\lightning_logs\\version_8\\checkpoints\\epoch=1-step=714.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f3328959e754595bc2a2ca6d9ba9037"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m        test_acc         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6284551620483398    \u001B[0m\u001B[35m \u001B[0m│\n│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   1.9903010129928589    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6284551620483398     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.9903010129928589     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type                      | Params\n",
      "------------------------------------------------------------\n",
      "0 | conv          | Conv1d                    | 525 K \n",
      "1 | lstm          | LSTM                      | 79.4 K\n",
      "2 | hidden2label  | Linear                    | 1.5 K \n",
      "3 | softmax       | Softmax                   | 0     \n",
      "4 | train_acc     | MulticlassAccuracy        | 0     \n",
      "5 | valid_acc     | MulticlassAccuracy        | 0     \n",
      "6 | test_acc      | MulticlassAccuracy        | 0     \n",
      "7 | test_conf_mat | MulticlassConfusionMatrix | 0     \n",
      "------------------------------------------------------------\n",
      "606 K     Trainable params\n",
      "0         Non-trainable params\n",
      "606 K     Total params\n",
      "2.425     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67a89f4865fd4797a662a150faec5ed2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e8826792164439689b1baf51d11a989"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000217D39B0E50>\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Studies\\DataScience-1sem\\DeepLearning\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"E:\\Studies\\DataScience-1sem\\DeepLearning\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1424, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m lightning\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, callbacks\u001B[38;5;241m=\u001B[39m[early_stopping])\n\u001B[0;32m      7\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mfit(model, train_dataloaders\u001B[38;5;241m=\u001B[39mtrain_dataset_loader, val_dataloaders\u001B[38;5;241m=\u001B[39mvalid_dataset_loader)\n\u001B[1;32m----> 8\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_dataset_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend(res[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     10\u001B[0m results\n",
      "File \u001B[1;32mE:\\Studies\\DataScience-1sem\\DeepLearning\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:706\u001B[0m, in \u001B[0;36mTrainer.test\u001B[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001B[0m\n\u001B[0;32m    704\u001B[0m     model \u001B[38;5;241m=\u001B[39m _maybe_unwrap_optimized(model)\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m_lightning_module \u001B[38;5;241m=\u001B[39m model\n\u001B[1;32m--> 706\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    707\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_test_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\n\u001B[0;32m    708\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Studies\\DataScience-1sem\\DeepLearning\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32mE:\\Studies\\DataScience-1sem\\DeepLearning\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:746\u001B[0m, in \u001B[0;36mTrainer._test_impl\u001B[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001B[0m\n\u001B[0;32m    743\u001B[0m \u001B[38;5;66;03m# links data to the trainer\u001B[39;00m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39mattach_data(model, test_dataloaders\u001B[38;5;241m=\u001B[39mdataloaders, datamodule\u001B[38;5;241m=\u001B[39mdatamodule)\n\u001B[1;32m--> 746\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_checkpoint_connector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_select_ckpt_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    747\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_provided\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_provided\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_connected\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[0;32m    748\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    749\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[0;32m    750\u001B[0m \u001B[38;5;66;03m# remove the tensors from the test results\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Studies\\DataScience-1sem\\DeepLearning\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py:107\u001B[0m, in \u001B[0;36m_CheckpointConnector._select_ckpt_path\u001B[1;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001B[0m\n\u001B[0;32m    105\u001B[0m         ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 107\u001B[0m     ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parse_ckpt_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_provided\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_provided\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_connected\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_connected\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ckpt_path\n",
      "File \u001B[1;32mE:\\Studies\\DataScience-1sem\\DeepLearning\\venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py:174\u001B[0m, in \u001B[0;36m_CheckpointConnector._parse_ckpt_path\u001B[1;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001B[0m\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mfast_dev_run:\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    171\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou cannot execute `.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m(ckpt_path=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)` with `fast_dev_run=True`.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    172\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Please pass an exact checkpoint path to `.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m(ckpt_path=...)`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    173\u001B[0m         )\n\u001B[1;32m--> 174\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    175\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m(ckpt_path=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)` is set but `ModelCheckpoint` is not configured to save the best model.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    176\u001B[0m     )\n\u001B[0;32m    177\u001B[0m \u001B[38;5;66;03m# load best weights\u001B[39;00m\n\u001B[0;32m    178\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mcheckpoint_callback, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_model_path\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mValueError\u001B[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
     ]
    }
   ],
   "source": [
    "early_stopping = lightning.pytorch.callbacks.EarlyStopping('val_loss')\n",
    "results = []\n",
    "predictions = []\n",
    "for i in range(5):\n",
    "    lightning.pytorch.seed_everything(i)\n",
    "    model = MyLSTM(513, 32,  128, 32, 8, 12)\n",
    "    trainer = lightning.Trainer(max_epochs=2, callbacks=[early_stopping])\n",
    "    trainer.fit(model, train_dataloaders=train_dataset_loader, val_dataloaders=valid_dataset_loader)\n",
    "    res = trainer.test(dataloaders=test_dataset_loader)\n",
    "    test_pred_tensor = torch.cat(trainer.predict(dataloaders=test_dataset_loader))\n",
    "    results.append(res[0])\n",
    "    predictions.append(test_pred_tensor)\n",
    "torch.save(torch.stack(predictions), \"spectogram_cnn_lstm_predictions.ts\")\n",
    "pd.DataFrame(results).to_csv(\"spectogram_cnn_lstm_metrics.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T22:27:51.969076Z",
     "end_time": "2023-04-19T22:27:52.168074Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
