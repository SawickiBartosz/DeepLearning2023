{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:12.125765Z",
     "end_time": "2023-04-24T22:23:15.824144Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import losses, optimizers, metrics, callbacks, Model, layers, backend as K\n",
    "import SpeechModels\n",
    "from augment_layers import FreqMaskLayer, TimeMaskLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:15.826137Z",
     "end_time": "2023-04-24T22:23:16.431700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "N_CLASS = 12\n",
    "MAX_EPOCHS = 200\n",
    "TEST_PATH = \"tensorflow-speech-recognition-challenge/test\"\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:16.433694Z",
     "end_time": "2023-04-24T22:23:16.453628Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46564 files belonging to 12 classes.\n",
      "Found 5919 files belonging to 12 classes.\n",
      "Found 12640 files belonging to 12 classes.\n",
      "label names: ['down' 'go' 'left' 'no' 'off' 'on' 'right' 'silence' 'stop' 'unknown'\n",
      " 'up' 'yes']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"group_data/train\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"group_data/val\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"group_data/test\",\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print(\"label names:\", label_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:16.449641Z",
     "end_time": "2023-04-24T22:23:21.831371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 158538 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=TEST_PATH,\n",
    "    batch_size=512,\n",
    "    output_sequence_length=16000,\n",
    "    shuffle=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:21.834361Z",
     "end_time": "2023-04-24T22:23:36.679472Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['audio']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.class_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:36.680470Z",
     "end_time": "2023-04-24T22:23:36.696416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorSpec(shape=(None, 16000, None), dtype=tf.float32, name=None),\n TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_ds.concatenate(val_ds)\n",
    "train_dataset.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:36.695420Z",
     "end_time": "2023-04-24T22:23:36.741266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorSpec(shape=(None, 16000, None), dtype=tf.float32, name=None),\n TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = test_ds\n",
    "validation_dataset.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:36.712363Z",
     "end_time": "2023-04-24T22:23:36.741266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 16000)]      0           []                               \n",
      "                                                                                                  \n",
      " normalized_spectrogram_model (  (None, 125, 80)     0           ['input[0][0]']                  \n",
      " Functional)                                                                                      \n",
      "                                                                                                  \n",
      " freq_mask_layer (FreqMaskLayer  (None, 125, 80)     0           ['normalized_spectrogram_model[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 125, 80, 1)   0           ['freq_mask_layer[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 125, 80, 10)  60          ['tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 125, 80, 10)  40         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 125, 80, 1)   51          ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 125, 80, 1)  4           ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " squeeze_last_dim (Lambda)      (None, 125, 80)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 125, 128)     74240       ['squeeze_last_dim[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 125, 128)    98816       ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 128)          0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 125)          0           ['dense[0][0]',                  \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " attSoftmax (Softmax)           (None, 125)          0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 128)          0           ['attSoftmax[0][0]',             \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 12)           396         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 200,455\n",
      "Trainable params: 200,433\n",
      "Non-trainable params: 22\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def squeeze(audio, labels):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, labels\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(squeeze, tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(squeeze, tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(squeeze, tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def create_model(freq=False, time=False):\n",
    "    m = SpeechModels.get_melspec_model(iLen=16000)\n",
    "    m.trainable = False\n",
    "    inputs, outputs = m.inputs, m.outputs\n",
    "\n",
    "    x = m(inputs)\n",
    "    if freq:\n",
    "        x = FreqMaskLayer(10)(x)\n",
    "    if time:\n",
    "        x = TimeMaskLayer(10)(x)\n",
    "    x = tf.expand_dims(x, axis=-1, name='mel_stft')\n",
    "\n",
    "    x = layers.Conv2D(10, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(1, (5, 1), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # x = Reshape((125, 80)) (x)\n",
    "    # keras.backend.squeeze(x, axis)\n",
    "    x = layers.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True)\n",
    "                             )(x)  # [b_s, seq_len, vec_dim]\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True)\n",
    "                             )(x)  # [b_s, seq_len, vec_dim]\n",
    "\n",
    "    x_first = layers.Lambda(lambda q: q[:, -1])(x)  # [b_s, vec_dim]\n",
    "    query = layers.Dense(128)(x_first)\n",
    "\n",
    "    # dot product attention\n",
    "    att_scores = layers.Dot(axes=[1, 2])([query, x])\n",
    "    att_scores = layers.Softmax(name='attSoftmax')(att_scores)  # [b_s, seq_len]\n",
    "\n",
    "    # rescale sequence\n",
    "    att_vector = layers.Dot(axes=[1, 1])([att_scores, x])  # [b_s, vec_dim]\n",
    "    x = layers.Dropout(rate=0.3)(att_vector)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(rate=0.3)(x)\n",
    "    x = layers.Dense(32)(x)\n",
    "    x = layers.Dropout(rate=0.3)(x)\n",
    "    output = layers.Dense(N_CLASS, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(freq=True)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:36.737280Z",
     "end_time": "2023-04-24T22:23:38.025121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(), metrics.SparseCategoricalCrossentropy()]\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='max',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=\"group_best_model_checkpoint\",\n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', factor=0.5, patience=3,\n",
    "                                        min_lr=0.00001, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:37.991234Z",
     "end_time": "2023-04-24T22:23:38.025121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.6580 - sparse_categorical_accuracy: 0.6185 - sparse_categorical_crossentropy: 1.6580"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 82s 664ms/step - loss: 1.6580 - sparse_categorical_accuracy: 0.6185 - sparse_categorical_crossentropy: 1.6580 - val_loss: 1.4998 - val_sparse_categorical_accuracy: 0.6383 - val_sparse_categorical_crossentropy: 1.4998 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 1.3458 - sparse_categorical_accuracy: 0.6438 - sparse_categorical_crossentropy: 1.3458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 67s 636ms/step - loss: 1.3458 - sparse_categorical_accuracy: 0.6438 - sparse_categorical_crossentropy: 1.3458 - val_loss: 0.9795 - val_sparse_categorical_accuracy: 0.7076 - val_sparse_categorical_crossentropy: 0.9795 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.7490 - sparse_categorical_accuracy: 0.7695 - sparse_categorical_crossentropy: 0.7490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 69s 658ms/step - loss: 0.7490 - sparse_categorical_accuracy: 0.7695 - sparse_categorical_crossentropy: 0.7490 - val_loss: 0.4948 - val_sparse_categorical_accuracy: 0.8483 - val_sparse_categorical_crossentropy: 0.4948 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.4854 - sparse_categorical_accuracy: 0.8557 - sparse_categorical_crossentropy: 0.4854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 69s 655ms/step - loss: 0.4854 - sparse_categorical_accuracy: 0.8557 - sparse_categorical_crossentropy: 0.4854 - val_loss: 0.3241 - val_sparse_categorical_accuracy: 0.9025 - val_sparse_categorical_crossentropy: 0.3241 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.3651 - sparse_categorical_accuracy: 0.8937 - sparse_categorical_crossentropy: 0.3651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 666ms/step - loss: 0.3651 - sparse_categorical_accuracy: 0.8937 - sparse_categorical_crossentropy: 0.3651 - val_loss: 0.2792 - val_sparse_categorical_accuracy: 0.9155 - val_sparse_categorical_crossentropy: 0.2792 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.2973 - sparse_categorical_accuracy: 0.9151 - sparse_categorical_crossentropy: 0.2973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 662ms/step - loss: 0.2973 - sparse_categorical_accuracy: 0.9151 - sparse_categorical_crossentropy: 0.2973 - val_loss: 0.2433 - val_sparse_categorical_accuracy: 0.9267 - val_sparse_categorical_crossentropy: 0.2433 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.2583 - sparse_categorical_accuracy: 0.9265 - sparse_categorical_crossentropy: 0.2583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 73s 697ms/step - loss: 0.2583 - sparse_categorical_accuracy: 0.9265 - sparse_categorical_crossentropy: 0.2583 - val_loss: 0.2297 - val_sparse_categorical_accuracy: 0.9308 - val_sparse_categorical_crossentropy: 0.2297 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.2276 - sparse_categorical_accuracy: 0.9360 - sparse_categorical_crossentropy: 0.2276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 670ms/step - loss: 0.2276 - sparse_categorical_accuracy: 0.9360 - sparse_categorical_crossentropy: 0.2276 - val_loss: 0.2167 - val_sparse_categorical_accuracy: 0.9356 - val_sparse_categorical_crossentropy: 0.2167 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.2038 - sparse_categorical_accuracy: 0.9429 - sparse_categorical_crossentropy: 0.2038"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 665ms/step - loss: 0.2038 - sparse_categorical_accuracy: 0.9429 - sparse_categorical_crossentropy: 0.2038 - val_loss: 0.1946 - val_sparse_categorical_accuracy: 0.9427 - val_sparse_categorical_crossentropy: 0.1946 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1873 - sparse_categorical_accuracy: 0.9480 - sparse_categorical_crossentropy: 0.1873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 667ms/step - loss: 0.1873 - sparse_categorical_accuracy: 0.9480 - sparse_categorical_crossentropy: 0.1873 - val_loss: 0.1656 - val_sparse_categorical_accuracy: 0.9517 - val_sparse_categorical_crossentropy: 0.1656 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1719 - sparse_categorical_accuracy: 0.9524 - sparse_categorical_crossentropy: 0.1719"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 668ms/step - loss: 0.1719 - sparse_categorical_accuracy: 0.9524 - sparse_categorical_crossentropy: 0.1719 - val_loss: 0.1556 - val_sparse_categorical_accuracy: 0.9555 - val_sparse_categorical_crossentropy: 0.1556 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "103/103 [==============================] - 50s 469ms/step - loss: 0.1632 - sparse_categorical_accuracy: 0.9556 - sparse_categorical_crossentropy: 0.1632 - val_loss: 0.1652 - val_sparse_categorical_accuracy: 0.9544 - val_sparse_categorical_crossentropy: 0.1652 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "103/103 [==============================] - 52s 491ms/step - loss: 0.1530 - sparse_categorical_accuracy: 0.9578 - sparse_categorical_crossentropy: 0.1530 - val_loss: 0.1786 - val_sparse_categorical_accuracy: 0.9518 - val_sparse_categorical_crossentropy: 0.1786 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1435 - sparse_categorical_accuracy: 0.9601 - sparse_categorical_crossentropy: 0.1435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 72s 690ms/step - loss: 0.1435 - sparse_categorical_accuracy: 0.9601 - sparse_categorical_crossentropy: 0.1435 - val_loss: 0.1645 - val_sparse_categorical_accuracy: 0.9566 - val_sparse_categorical_crossentropy: 0.1645 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1363 - sparse_categorical_accuracy: 0.9627 - sparse_categorical_crossentropy: 0.1363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 74s 704ms/step - loss: 0.1363 - sparse_categorical_accuracy: 0.9627 - sparse_categorical_crossentropy: 0.1363 - val_loss: 0.1494 - val_sparse_categorical_accuracy: 0.9586 - val_sparse_categorical_crossentropy: 0.1494 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1245 - sparse_categorical_accuracy: 0.9662 - sparse_categorical_crossentropy: 0.1245"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 72s 679ms/step - loss: 0.1245 - sparse_categorical_accuracy: 0.9662 - sparse_categorical_crossentropy: 0.1245 - val_loss: 0.1594 - val_sparse_categorical_accuracy: 0.9600 - val_sparse_categorical_crossentropy: 0.1594 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "103/103 [==============================] - 51s 479ms/step - loss: 0.1249 - sparse_categorical_accuracy: 0.9667 - sparse_categorical_crossentropy: 0.1249 - val_loss: 0.1518 - val_sparse_categorical_accuracy: 0.9586 - val_sparse_categorical_crossentropy: 0.1518 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1170 - sparse_categorical_accuracy: 0.9688 - sparse_categorical_crossentropy: 0.1170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 73s 697ms/step - loss: 0.1170 - sparse_categorical_accuracy: 0.9688 - sparse_categorical_crossentropy: 0.1170 - val_loss: 0.1464 - val_sparse_categorical_accuracy: 0.9624 - val_sparse_categorical_crossentropy: 0.1464 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1100 - sparse_categorical_accuracy: 0.9701 - sparse_categorical_crossentropy: 0.1100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 72s 689ms/step - loss: 0.1100 - sparse_categorical_accuracy: 0.9701 - sparse_categorical_crossentropy: 0.1100 - val_loss: 0.1456 - val_sparse_categorical_accuracy: 0.9627 - val_sparse_categorical_crossentropy: 0.1456 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "103/103 [==============================] - 52s 488ms/step - loss: 0.1065 - sparse_categorical_accuracy: 0.9705 - sparse_categorical_crossentropy: 0.1065 - val_loss: 0.1680 - val_sparse_categorical_accuracy: 0.9585 - val_sparse_categorical_crossentropy: 0.1680 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "103/103 [==============================] - 50s 475ms/step - loss: 0.1041 - sparse_categorical_accuracy: 0.9719 - sparse_categorical_crossentropy: 0.1041 - val_loss: 0.1507 - val_sparse_categorical_accuracy: 0.9600 - val_sparse_categorical_crossentropy: 0.1507 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0969 - sparse_categorical_accuracy: 0.9734 - sparse_categorical_crossentropy: 0.0969\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "103/103 [==============================] - 51s 484ms/step - loss: 0.0969 - sparse_categorical_accuracy: 0.9734 - sparse_categorical_crossentropy: 0.0969 - val_loss: 0.1457 - val_sparse_categorical_accuracy: 0.9626 - val_sparse_categorical_crossentropy: 0.1457 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0864 - sparse_categorical_accuracy: 0.9766 - sparse_categorical_crossentropy: 0.0864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 88s 848ms/step - loss: 0.0864 - sparse_categorical_accuracy: 0.9766 - sparse_categorical_crossentropy: 0.0864 - val_loss: 0.1366 - val_sparse_categorical_accuracy: 0.9642 - val_sparse_categorical_crossentropy: 0.1366 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0809 - sparse_categorical_accuracy: 0.9782 - sparse_categorical_crossentropy: 0.0809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 77s 734ms/step - loss: 0.0809 - sparse_categorical_accuracy: 0.9782 - sparse_categorical_crossentropy: 0.0809 - val_loss: 0.1395 - val_sparse_categorical_accuracy: 0.9646 - val_sparse_categorical_crossentropy: 0.1395 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "103/103 [==============================] - 53s 494ms/step - loss: 0.0759 - sparse_categorical_accuracy: 0.9790 - sparse_categorical_crossentropy: 0.0759 - val_loss: 0.1511 - val_sparse_categorical_accuracy: 0.9635 - val_sparse_categorical_crossentropy: 0.1511 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "103/103 [==============================] - 53s 497ms/step - loss: 0.0740 - sparse_categorical_accuracy: 0.9804 - sparse_categorical_crossentropy: 0.0740 - val_loss: 0.1511 - val_sparse_categorical_accuracy: 0.9634 - val_sparse_categorical_crossentropy: 0.1511 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0744 - sparse_categorical_accuracy: 0.9802 - sparse_categorical_crossentropy: 0.0744\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "103/103 [==============================] - 52s 488ms/step - loss: 0.0744 - sparse_categorical_accuracy: 0.9802 - sparse_categorical_crossentropy: 0.0744 - val_loss: 0.1500 - val_sparse_categorical_accuracy: 0.9637 - val_sparse_categorical_crossentropy: 0.1500 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0660 - sparse_categorical_accuracy: 0.9825 - sparse_categorical_crossentropy: 0.0660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 71s 679ms/step - loss: 0.0660 - sparse_categorical_accuracy: 0.9825 - sparse_categorical_crossentropy: 0.0660 - val_loss: 0.1430 - val_sparse_categorical_accuracy: 0.9661 - val_sparse_categorical_crossentropy: 0.1430 - lr: 2.5000e-04\n",
      "Epoch 29/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0636 - sparse_categorical_accuracy: 0.9829 - sparse_categorical_crossentropy: 0.0636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 671ms/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9829 - sparse_categorical_crossentropy: 0.0636 - val_loss: 0.1419 - val_sparse_categorical_accuracy: 0.9667 - val_sparse_categorical_crossentropy: 0.1419 - lr: 2.5000e-04\n",
      "Epoch 30/200\n",
      "103/103 [==============================] - 50s 470ms/step - loss: 0.0635 - sparse_categorical_accuracy: 0.9829 - sparse_categorical_crossentropy: 0.0635 - val_loss: 0.1440 - val_sparse_categorical_accuracy: 0.9657 - val_sparse_categorical_crossentropy: 0.1440 - lr: 2.5000e-04\n",
      "Epoch 31/200\n",
      "103/103 [==============================] - 49s 459ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9833 - sparse_categorical_crossentropy: 0.0633 - val_loss: 0.1438 - val_sparse_categorical_accuracy: 0.9661 - val_sparse_categorical_crossentropy: 0.1438 - lr: 2.5000e-04\n",
      "Epoch 32/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0622 - sparse_categorical_accuracy: 0.9841 - sparse_categorical_crossentropy: 0.0622\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "103/103 [==============================] - 50s 474ms/step - loss: 0.0622 - sparse_categorical_accuracy: 0.9841 - sparse_categorical_crossentropy: 0.0622 - val_loss: 0.1455 - val_sparse_categorical_accuracy: 0.9651 - val_sparse_categorical_crossentropy: 0.1455 - lr: 2.5000e-04\n",
      "Epoch 33/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0597 - sparse_categorical_accuracy: 0.9846 - sparse_categorical_crossentropy: 0.0597"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: group_best_model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 70s 668ms/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9846 - sparse_categorical_crossentropy: 0.0597 - val_loss: 0.1434 - val_sparse_categorical_accuracy: 0.9671 - val_sparse_categorical_crossentropy: 0.1434 - lr: 1.2500e-04\n",
      "Epoch 34/200\n",
      "103/103 [==============================] - 51s 481ms/step - loss: 0.0580 - sparse_categorical_accuracy: 0.9848 - sparse_categorical_crossentropy: 0.0580 - val_loss: 0.1448 - val_sparse_categorical_accuracy: 0.9652 - val_sparse_categorical_crossentropy: 0.1448 - lr: 1.2500e-04\n",
      "Epoch 35/200\n",
      "103/103 [==============================] - 49s 458ms/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9847 - sparse_categorical_crossentropy: 0.0584 - val_loss: 0.1432 - val_sparse_categorical_accuracy: 0.9669 - val_sparse_categorical_crossentropy: 0.1432 - lr: 1.2500e-04\n",
      "Epoch 36/200\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.0593 - sparse_categorical_accuracy: 0.9845 - sparse_categorical_crossentropy: 0.0593\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "103/103 [==============================] - 49s 462ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9845 - sparse_categorical_crossentropy: 0.0593 - val_loss: 0.1434 - val_sparse_categorical_accuracy: 0.9657 - val_sparse_categorical_crossentropy: 0.1434 - lr: 1.2500e-04\n",
      "Epoch 37/200\n",
      "103/103 [==============================] - 48s 461ms/step - loss: 0.0552 - sparse_categorical_accuracy: 0.9855 - sparse_categorical_crossentropy: 0.0552 - val_loss: 0.1434 - val_sparse_categorical_accuracy: 0.9665 - val_sparse_categorical_crossentropy: 0.1434 - lr: 6.2500e-05\n",
      "Epoch 38/200\n",
      "103/103 [==============================] - 48s 460ms/step - loss: 0.0559 - sparse_categorical_accuracy: 0.9856 - sparse_categorical_crossentropy: 0.0559 - val_loss: 0.1437 - val_sparse_categorical_accuracy: 0.9663 - val_sparse_categorical_crossentropy: 0.1437 - lr: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:23:38.018144Z",
     "end_time": "2023-04-24T23:03:13.303298Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 43s 134ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:03:13.304294Z",
     "end_time": "2023-04-24T23:03:56.339136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "predict_class = np.array(list(label_names[i] for i in np.argmax(predictions, axis=1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:03:56.341130Z",
     "end_time": "2023-04-24T23:03:56.448777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "for i, (root, dir, fname) in enumerate(os.walk(os.path.join(TEST_PATH, 'audio'))):\n",
    "    files = np.array(fname)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:03:56.451768Z",
     "end_time": "2023-04-24T23:03:56.821534Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.vstack([files, predict_class]).T, columns=[\"fname\", \"label\"])\n",
    "submission.to_csv(\"group_submission.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T23:03:56.823527Z",
     "end_time": "2023-04-24T23:03:57.114042Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
